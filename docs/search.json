[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Dianne H. Cook",
    "section": "",
    "text": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective\n\n\n\n\n\n\nR\n\n\nhigh-dimensional data\n\n\ndeep learning\n\n\ndata visualisation\n\n\nstatistical graphics\n\n\ndimension reduction\n\n\ntours\n\n\ndata mining\n\n\n\n\n\n\n\n\n\n11 March 2025\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is EDA, really?\n\n\n\n\n\n\nR\n\n\ndata science\n\n\ndescriptive statistics\n\n\nexploratory data analysis\n\n\n\n\n\n\n\n\n\n26 August 2024\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nTinkering with the core R code is easier than it used to be, thanks to R Dev Container\n\n\n\n\n\n\nR\n\n\nopen source software\n\n\nbug fixes\n\n\ncommunity\n\n\n\n\n\n\n\n\n\n14 July 2024\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nHumans all the way down: statistical reflection and self-critique for interactive data analysis\n\n\n\n\n\n\ninteractive data analysis\n\n\nstatistics\n\n\ndata science\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n25 June 2024\n\n\nDi Cook, Rachel Franklin, Cagatay Turkay, Mari-Cruz Villa-Uriol, Levi Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nHexmaps with sugarbag make it easier to see the electoral map\n\n\n\n\n\n\ndata visualisation\n\n\nstatistics\n\n\nteaching\n\n\nspatial data\n\n\n\n\n\n\n\n\n\n21 May 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use a tour to check if your model suffers from multicollinearity\n\n\n\n\n\n\ndata visualisation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n13 September 2019\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nThe background to useR! 2018\n\n\n\n\n\n\n\n\n\n\n\n10 October 2018\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nGetting past the little hiccups to getting plotly animations into slides\n\n\n\n\n\n\ndata visualisation\n\n\nslides\n\n\n\n\n\n\n\n\n\n29 August 2018\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing my energy usage\n\n\n\n\n\n\ndata visualisation\n\n\nR\n\n\ntidyverse\n\n\ntime series\n\n\nsmart meter\n\n\n\n\n\n\n\n\n\n22 April 2018\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nRookie mistakes and how to fix them when making plots of data\n\n\n\n\n\n\neducation\n\n\nstatistical graphics\n\n\ndata visualisation\n\n\nR\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n14 April 2018\n\n\nStuart Lee, Di Cook\n\n\n\n\n\n\n\n\n\n\n\n\nBetter cricket plots\n\n\n\n\n\n\n\n\n\n\n\n26 December 2015\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Sciences, Cornell University\n\n\n\n\n\n\n\n\n\n\n\n6 November 2015\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for Statistics and Applications in Forensic Evidence\n\n\n\n\n\n\n\n\n\n\n\n26 October 2015\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nData Science for Managers, 2015, Monash Conference Center, Melbourne\n\n\n\n\n\n\n\n\n\n\n\n13 October 2015\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nGraduates in Statistical Graphics Research at ISU 2015\n\n\n\n\n\n\n\n\n\n\n\n17 May 2015\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nEDA at the UN\n\n\n\n\n\n\n\n\n\n\n\n16 November 2014\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nNew version of nullabor package released\n\n\n\n\n\n\n\n\n\n\n\n3 November 2014\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical computing research\n\n\n\n\n\n\n\n\n\n\n\n5 October 2014\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nHow good is Nick Kygrios?\n\n\n\n\n\n\n\n\n\n\n\n28 September 2014\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nFacetted barcharts, and fluctuation diagrams are good alternatives to stacked barcharts\n\n\n\n\n\n\n\n\n\n\n\n25 September 2014\n\n\nDi Cook\n\n\n\n\n\n\n\n\n\n\n\n\nA Graphical Expedition into a Statistics Gradebook\n\n\n\n\n\n\n\n\n\n\n\n13 September 2014\n\n\nDi Cook\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "files/ACEMS_retreat_2020/components/endslide.html",
    "href": "files/ACEMS_retreat_2020/components/endslide.html",
    "title": "Thanks for listening!",
    "section": "",
    "text": "background-image: url() background-size: cover class: hide-slide-number split-70 count: false\n.column.shade_black[.content["
  },
  {
    "objectID": "files/ACEMS_retreat_2020/components/endslide.html#acknowledgements",
    "href": "files/ACEMS_retreat_2020/components/endslide.html#acknowledgements",
    "title": "Thanks for listening!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSlides produced using Rmarkdown with xaringan styling. Monash style by the kunoichi, Dr Emi Tanaka.\nData and code for analysis is available on Weihao’s GitHub repo. Shiny app code is available at Chang’s GitHub repo."
  },
  {
    "objectID": "files/MalaysiaR/components/endslide.html",
    "href": "files/MalaysiaR/components/endslide.html",
    "title": "Thanks for listening!",
    "section": "",
    "text": "background-image: url() background-size: cover class: hide-slide-number split-70 count: false\n.column.shade_black[.content["
  },
  {
    "objectID": "files/MalaysiaR/components/endslide.html#acknowledgements",
    "href": "files/MalaysiaR/components/endslide.html#acknowledgements",
    "title": "Thanks for listening!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nhttps://bit.ly/Cook-MalaysiaR\nSlides produced using Rmarkdown with xaringan styling. Monash style by the kunoichi, Dr Emi Tanaka.\nspotoroo package is available on CRAN and Patrick’s GitHub repo. Current developments are available at bushyr GitHub repo."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "home",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "links.html#quarto",
    "href": "links.html#quarto",
    "title": "home",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "NOTE: You’ll find most of these papers as pre-prints on arxiv.\n\n\n\n\n\n\n\n\ntitle\nyear\njournal\n\n\n\n\nDemonstrating the Capabilities of the lionfish Software for Interactive Visualization of Market Segmentation Partitions\n2025\nAustrian Journal of Statistics\n\n\nIs This Normal? A New Projection Pursuit Index to Assess a Sample Against a Multivariate Null Distribution\n2025\nJournal of Computational and Graphical Statistics\n\n\nDesigning the Australian Cancer Atlas: visualizing geostatistical model uncertainty for multiple audiences\n2024\nJournal of the American Medical Informatics Association\n\n\nA Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol\n2024\nJournal of Computational and Graphical Statistics\n\n\nA Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data\n2024\nJournal of Computational and Graphical Statistics\n\n\nPerformance Is Not Enough: The Story Told by a Rashomon Quartet\n2024\nJournal of Computational and Graphical Statistics\n\n\nExploring local explanations of nonlinear models using animated linear projections\n2024\nComputational Statistics\n\n\nA Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely\n2023\nThe R Journal\n\n\nA Hexagon Tile Map Algorithm for Displaying Spatial Data\n2023\nThe R Journal\n\n\nNew and Simplified Manual Controls for Projection and Slice Tours, With Application to Exploring Classification Boundaries in High Dimensions\n2023\nJournal of Computational and Graphical Statistics\n\n\nInteractive graphics for visually diagnosing forest classifiers in R\n2023\nComputational Statistics\n\n\nExpanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations\n2023\nJournal of Statistical Software\n\n\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\n2022\nThe R Journal\n\n\nCommentary on “Visualization in Operations Management Research”: Incorporating Statistical Thinking into Visualization Practices for Decision Making in Operational Management\n2022\nINFORMS Journal on Data Science\n\n\nA Journey from Wild to Textbook Data to Reproducibly Refresh the Wages Data from the National Longitudinal Survey of Youth Database\n2022\nJournal of Statistics and Data Science Education\n\n\nHole or Grain? A Section Pursuit Index for Finding Hidden Structure in Multiple Dimensions\n2022\nJournal of Computational and Graphical Statistics\n\n\nThe state‐of‐the‐art on tours for dynamic visualization of high‐dimensional data\n2022\nWIREs Computational Statistics\n\n\nCasting multiple shadows: interactive data visualisation with tours and embeddings\n2022\nJournal of Data Science, Statistics, and Visualisation\n\n\nBurning Sage: Reversing the Curse of Dimensionality in the Visualization of High-Dimensional Data\n2022\nJournal of Computational and Graphical Statistics\n\n\nVisualizing Probability Distributions Across Bivariate Cyclic Temporal Granularities\n2022\nJournal of Computational and Graphical Statistics\n\n\nNanoMethViz: An R/Bioconductor package for visualizing long-read methylation data\n2021\nPLOS Computational Biology\n\n\nA Projection Pursuit Forest Algorithm for Supervised Classification\n2021\nJournal of Computational and Graphical Statistics\n\n\nASAS-NANP SYMPOSIUM: prospects for interactive and dynamic graphics in the era of data-rich animal science1\n2021\nJournal of Animal Science\n\n\nNanoMethViz: an R/Bioconductor package for visualizing long-read methylation data\n2021\nNA\n\n\nA Study on Student Performance, Engagement, and Experience With Kaggle InClass data Challenges\n2021\nJournal of Statistics and Data Science Education\n\n\nConversations in Time: Interactive Visualization to Explore Structured Temporal Data\n2021\nThe R Journal\n\n\nEditorial\n2021\nR Journal\n\n\nVisual Diagnostics for Constrained Optimisation with Application to Guided Tours\n2021\nThe R Journal\n\n\nVisual Diagnostics for Constrained Optimisation with Application to Guided Tours\n2021\nThe R Journal\n\n\nbigPint: A Bioconductor visualization package that makes big data pint-sized\n2020\nPLOS Computational Biology\n\n\nspinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data\n2020\nThe R Journal\n\n\nThe 2013 Data Expo of the American Statistical Association\n2019\nComputational Statistics\n\n\nConnecting R with D3 for dynamic graphics, to explore multivariate data with tours\n2019\nThe R Journal\n\n\nPlyranges: A grammar of genomic data transformation\n2019\nGenome Biology\n\n\nplyranges: A grammar of genomic data transformation\n2018\nNA\n\n\nDynamical projections for the visualization of PDFSense data\n2018\nEuropean Physical Journal C\n\n\nMeasuring Lineup Difficulty By Matching Distance Metrics With Subject Choices in Crowd-Sourced Data\n2018\nJournal of Computational and Graphical Statistics\n\n\nCongruent biogeographical disjunctions at a continent-wide scale: Quantifying and clarifying the role of biogeographic barriers in the Australian tropics\n2017\nPLoS ONE\n\n\nForwards column\n2017\nR Journal\n\n\nModel Choice and Diagnostics for Linear Mixed-Effects Models Using Statistics on Street Corners\n2017\nJournal of Computational and Graphical Statistics\n\n\nThe twentieth-century computer graphics revolution in statistics\n2017\nVisible Numbers: Essays on the History of Statistical Graphics\n\n\nData Visualization and Statistical Graphics in Big Data Analysis\n2016\nAnnual Review of Statistics and Its Application\n\n\nEnabling Interactivity on Displays of Multivariate Time Series and Longitudinal Data\n2016\nJournal of Computational and Graphical Statistics\n\n\nEscape from Boxland\n2016\nThe R Journal\n\n\nAuthors’ response to discussants\n2015\nStatistical Analysis and Data Mining\n\n\nDoes host-plant diversity explain species richness in insects? A test using Coccidae (Hemiptera)\n2015\nEcological Entomology\n\n\nUsing visual statistical inference to better understand random class separations in high dimension, low sample size data\n2015\nComputational Statistics\n\n\nVisualizing communication patterns at DinoFun World\n2015\n2015 IEEE Conference on Visual Analytics Science and Technology, VAST 2015 - Proceedings\n\n\nVisualizing statistical models: Removing the blindfold\n2015\nStatistical Analysis and Data Mining\n\n\nVisually exploring missing values in multivariable data using a graphical user interface\n2015\nJournal of Statistical Software\n\n\nFour papers on contemporary software design strategies for statistical methodologists\n2014\nStatistical Science\n\n\nIdentification of candidate genes involved in early iron deficiency chlorosis signaling in soybean (Glycine max) roots and leaves\n2014\nBMC Genomics\n\n\nReplication protein A subunit 3 and the iron efficiency response in soybean\n2014\nPlant, Cell and Environment\n\n\nThe 2011 data Expo of the American Statistical Association\n2014\nComputational Statistics\n\n\nAn algorithm for deciding the number of clusters and validation using simulated data with application to exploring crop population structure\n2013\nAnnals of Applied Statistics\n\n\nGradient-based habitat affinities predict species vulnerability to drought\n2013\nEcology\n\n\nHIV dynamics impacting the efficacy of HIV/AIDS treatments\n2013\nJournal of Proteomics and Bioinformatics\n\n\nIn tennis, do smashes win matches?\n2013\nSignificance\n\n\nLet graphics tell the story-Datasets in R\n2013\nR Journal\n\n\nPPtree: Projection pursuit classification tree\n2013\nElectronic Journal of Statistics\n\n\nThe generalized pairs plot\n2013\nJournal of Computational and Graphical Statistics\n\n\nValidation of visual statistical inference, applied to linear models\n2013\nJournal of the American Statistical Association\n\n\nBirdsEyeView (BEV): graphical overviews of experimental data.\n2012\nBMC bioinformatics\n\n\nGlyph-maps for visually exploring temporal patterns in climate data and models\n2012\nEnvironmetrics\n\n\nGraphical tests for power comparison of competing designs\n2012\nIEEE Transactions on Visualization and Computer Graphics\n\n\nggbio: an R package for extending the grammar of graphics for genomic data\n2012\nGenome biology\n\n\ntourrGui: A gWidgets GUI for the tour to explore high-dimensional data using low-dimensional projections\n2012\nJournal of Statistical Software\n\n\nDelayed, Canceled, on Time, Boarding… Flying in the USA\n2011\nJournal of Computational and Graphical Statistics\n\n\nPKgraph: An R package for graphically diagnosing population pharmacokinetic models\n2011\nComputer Methods and Programs in Biomedicine\n\n\nPopulation structure and linkage disequilibrium in oat (Avena sativa L.): Implications for genome-wide association studies\n2011\nTheoretical and Applied Genetics\n\n\nTips for presenting your work\n2011\nR Journal\n\n\nTourr: An R package for exploring multivariate data with projections\n2011\nJournal of Statistical Software\n\n\nA projection pursuit index for large p small n data\n2010\nStatistics and Computing\n\n\nEditorial: Publishing animations, 3D visualizations, and movies in JCGS\n2010\nJournal of Computational and Graphical Statistics\n\n\nGlaciers melt as mountains warm: A graphical case study\n2010\nComputational Statistics\n\n\nGraphical inference for infovis\n2010\nIEEE Transactions on Visualization and Computer Graphics\n\n\nExtending the GGobi pipeline from R : RRapid prototyping of interactive visualizations\n2009\nComputational Statistics\n\n\nIncorporating exploratory methods using dynamic graphics into multivariate statistics classes: Curriculum development\n2009\nQuality Research in Literacy and Science Education: International Perspectives and Gold Standards\n\n\nStatistical inference for exploratory data analysis and model diagnostics\n2009\nPhilosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences\n\n\nThe plumbing of interactive graphics\n2009\nComputational Statistics\n\n\nComment\n2008\nTechnometrics\n\n\nExplorase: Multivariate exploratory analysis and visualization for systems biology\n2008\nJournal of Statistical Software\n\n\nVisual methods for examining SVM classifiers\n2008\nLecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n\n\nexploRase: Exploratory data analysis of systems biology data\n2006\nProceedings - Fourth International Conference on Coordinated and Multiple Views in Exploratory Visualization, CMV 2006\n\n\nBarleyBase - An expression profiling database for plant genomics\n2005\nNucleic Acids Research\n\n\nLimn matrix: A tool for visualizing large, distributed, high-dimension data sets\n2005\n18th International Conference on Computer Applications in Industry and Engineering 2005, CAINE 2005\n\n\nProjection pursuit for exploratory supervised classification\n2005\nJournal of Computational and Graphical Statistics\n\n\nComputational Methods for High-Dimensional Rotations in Data Visualization\n2004\nHandbook of Statistics\n\n\nA Projection Pursuit Method on the Multidimensional Squared Contingency Table\n2003\nComputational Statistics\n\n\nGGobi: Evolving from XGobi into an extensible framework for interactive data visualization\n2003\nComputational Statistics and Data Analysis\n\n\nHigh Dimensional System Design using Genetic Algorithms & Visualization\n2003\nProceedings of the American Control Conference\n\n\nMetNet: Software to build and model the biogenetic lattice of Arabidopsis\n2003\nComparative and Functional Genomics\n\n\nTowards simple, easy-to-understand, yet accurate classifiers\n2003\nProceedings - IEEE International Conference on Data Mining, ICDM\n\n\nVisualising high-dimensional data in time and space: Ideas from the Orca project\n2002\nChemometrics and Intelligent Laboratory Systems\n\n\nGaining insights into support vector machine pattern classifiers using projection-based tour methods\n2001\nProceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n\n\nVisualizing membership in multiple clusters after fuzzy c-means clustering\n2001\nProceedings of SPIE - The International Society for Optical Engineering\n\n\nInteractive visualization of hierarchical clusters using MDS and MST\n2000\nMetrika\n\n\nLinking ArcView™ and XGobi: Insight behind the Front End\n2000\nJournal of Computational and Graphical Statistics\n\n\nOrca: A visualization toolkit for high-dimensional data\n2000\nJournal of Computational and Graphical Statistics\n\n\nVisual data mining in atmospheric science data\n2000\nData Mining and Knowledge Discovery\n\n\nVisualization and case reduction in multivariate data clustering\n2000\nProceedings of SPIE - The International Society for Optical Engineering\n\n\nVisualization of data\n2000\nCurrent Opinion in Biotechnology\n\n\nVisualization of stable manifolds and multidimensional surfaces in the analysis of power system dynamics\n2000\nJournal of Nonlinear Science\n\n\nBenefits of statistical visualization in an immersive environment\n1999\nProceedings - Virtual Reality Annual International Symposium\n\n\nXGobi vs the C2: Results of an experiment comparing data visualization in a 3-D immersive virtual reality environment with a 2-D workstation display\n1999\nComputational Statistics\n\n\nExploring environmental data in a highly immersive virtual reality environment\n1998\nEnvironmental Monitoring and Assessment\n\n\nXGobi: Interactive dynamic data visualization in the X window system\n1998\nJournal of Computational and Graphical Statistics\n\n\nBinning of Kernel-based projection pursuit indices in XGobi\n1997\nComputational Statistics and Data Analysis\n\n\nCalibrate your eyes to recognize high-dimensional shapes from their low-dimensional projections\n1997\nJournal of Statistical Software\n\n\nDynamic graphics in a GIS: More examples using linked software\n1997\nComputers and Geosciences\n\n\nManual controls for high-dimensional data projections\n1997\nJournal of Computational and Graphical Statistics\n\n\nDynamic graphics in a GIS: Exploring and analyzing multivariate spatial data using linked software\n1996\nComputational Statistics\n\n\nLinked ArcView 2.1 and XGobi environment - GIS, dynamic statistical graphics, and spatial data\n1996\nProceedings of the ACM Workshop on Advances in Geographic Information Systems\n\n\nUsing Graphics and Simulation to Teach Statistical Concepts\n1996\nAmerican Statistician\n\n\nGrand tour and projection pursuit\n1995\nJournal of Computational and Graphical Statistics\n\n\nPojection pursuit indexes based on orthonormal function expansions\n1993\nJournal of Computational and Graphical Statistics\n\n\n\n\n\n\n\n\nThis list is automatically populated from orcid.org"
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "",
    "text": "NOTE: You’ll find most of these papers as pre-prints on arxiv.\n\n\n\n\n\n\n\n\ntitle\nyear\njournal\n\n\n\n\nDemonstrating the Capabilities of the lionfish Software for Interactive Visualization of Market Segmentation Partitions\n2025\nAustrian Journal of Statistics\n\n\nIs This Normal? A New Projection Pursuit Index to Assess a Sample Against a Multivariate Null Distribution\n2025\nJournal of Computational and Graphical Statistics\n\n\nDesigning the Australian Cancer Atlas: visualizing geostatistical model uncertainty for multiple audiences\n2024\nJournal of the American Medical Informatics Association\n\n\nA Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol\n2024\nJournal of Computational and Graphical Statistics\n\n\nA Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data\n2024\nJournal of Computational and Graphical Statistics\n\n\nPerformance Is Not Enough: The Story Told by a Rashomon Quartet\n2024\nJournal of Computational and Graphical Statistics\n\n\nExploring local explanations of nonlinear models using animated linear projections\n2024\nComputational Statistics\n\n\nA Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely\n2023\nThe R Journal\n\n\nA Hexagon Tile Map Algorithm for Displaying Spatial Data\n2023\nThe R Journal\n\n\nNew and Simplified Manual Controls for Projection and Slice Tours, With Application to Exploring Classification Boundaries in High Dimensions\n2023\nJournal of Computational and Graphical Statistics\n\n\nInteractive graphics for visually diagnosing forest classifiers in R\n2023\nComputational Statistics\n\n\nExpanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations\n2023\nJournal of Statistical Software\n\n\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\n2022\nThe R Journal\n\n\nCommentary on “Visualization in Operations Management Research”: Incorporating Statistical Thinking into Visualization Practices for Decision Making in Operational Management\n2022\nINFORMS Journal on Data Science\n\n\nA Journey from Wild to Textbook Data to Reproducibly Refresh the Wages Data from the National Longitudinal Survey of Youth Database\n2022\nJournal of Statistics and Data Science Education\n\n\nHole or Grain? A Section Pursuit Index for Finding Hidden Structure in Multiple Dimensions\n2022\nJournal of Computational and Graphical Statistics\n\n\nThe state‐of‐the‐art on tours for dynamic visualization of high‐dimensional data\n2022\nWIREs Computational Statistics\n\n\nCasting multiple shadows: interactive data visualisation with tours and embeddings\n2022\nJournal of Data Science, Statistics, and Visualisation\n\n\nBurning Sage: Reversing the Curse of Dimensionality in the Visualization of High-Dimensional Data\n2022\nJournal of Computational and Graphical Statistics\n\n\nVisualizing Probability Distributions Across Bivariate Cyclic Temporal Granularities\n2022\nJournal of Computational and Graphical Statistics\n\n\nNanoMethViz: An R/Bioconductor package for visualizing long-read methylation data\n2021\nPLOS Computational Biology\n\n\nA Projection Pursuit Forest Algorithm for Supervised Classification\n2021\nJournal of Computational and Graphical Statistics\n\n\nASAS-NANP SYMPOSIUM: prospects for interactive and dynamic graphics in the era of data-rich animal science1\n2021\nJournal of Animal Science\n\n\nNanoMethViz: an R/Bioconductor package for visualizing long-read methylation data\n2021\nNA\n\n\nA Study on Student Performance, Engagement, and Experience With Kaggle InClass data Challenges\n2021\nJournal of Statistics and Data Science Education\n\n\nConversations in Time: Interactive Visualization to Explore Structured Temporal Data\n2021\nThe R Journal\n\n\nEditorial\n2021\nR Journal\n\n\nVisual Diagnostics for Constrained Optimisation with Application to Guided Tours\n2021\nThe R Journal\n\n\nVisual Diagnostics for Constrained Optimisation with Application to Guided Tours\n2021\nThe R Journal\n\n\nbigPint: A Bioconductor visualization package that makes big data pint-sized\n2020\nPLOS Computational Biology\n\n\nspinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data\n2020\nThe R Journal\n\n\nThe 2013 Data Expo of the American Statistical Association\n2019\nComputational Statistics\n\n\nConnecting R with D3 for dynamic graphics, to explore multivariate data with tours\n2019\nThe R Journal\n\n\nPlyranges: A grammar of genomic data transformation\n2019\nGenome Biology\n\n\nplyranges: A grammar of genomic data transformation\n2018\nNA\n\n\nDynamical projections for the visualization of PDFSense data\n2018\nEuropean Physical Journal C\n\n\nMeasuring Lineup Difficulty By Matching Distance Metrics With Subject Choices in Crowd-Sourced Data\n2018\nJournal of Computational and Graphical Statistics\n\n\nCongruent biogeographical disjunctions at a continent-wide scale: Quantifying and clarifying the role of biogeographic barriers in the Australian tropics\n2017\nPLoS ONE\n\n\nForwards column\n2017\nR Journal\n\n\nModel Choice and Diagnostics for Linear Mixed-Effects Models Using Statistics on Street Corners\n2017\nJournal of Computational and Graphical Statistics\n\n\nThe twentieth-century computer graphics revolution in statistics\n2017\nVisible Numbers: Essays on the History of Statistical Graphics\n\n\nData Visualization and Statistical Graphics in Big Data Analysis\n2016\nAnnual Review of Statistics and Its Application\n\n\nEnabling Interactivity on Displays of Multivariate Time Series and Longitudinal Data\n2016\nJournal of Computational and Graphical Statistics\n\n\nEscape from Boxland\n2016\nThe R Journal\n\n\nAuthors’ response to discussants\n2015\nStatistical Analysis and Data Mining\n\n\nDoes host-plant diversity explain species richness in insects? A test using Coccidae (Hemiptera)\n2015\nEcological Entomology\n\n\nUsing visual statistical inference to better understand random class separations in high dimension, low sample size data\n2015\nComputational Statistics\n\n\nVisualizing communication patterns at DinoFun World\n2015\n2015 IEEE Conference on Visual Analytics Science and Technology, VAST 2015 - Proceedings\n\n\nVisualizing statistical models: Removing the blindfold\n2015\nStatistical Analysis and Data Mining\n\n\nVisually exploring missing values in multivariable data using a graphical user interface\n2015\nJournal of Statistical Software\n\n\nFour papers on contemporary software design strategies for statistical methodologists\n2014\nStatistical Science\n\n\nIdentification of candidate genes involved in early iron deficiency chlorosis signaling in soybean (Glycine max) roots and leaves\n2014\nBMC Genomics\n\n\nReplication protein A subunit 3 and the iron efficiency response in soybean\n2014\nPlant, Cell and Environment\n\n\nThe 2011 data Expo of the American Statistical Association\n2014\nComputational Statistics\n\n\nAn algorithm for deciding the number of clusters and validation using simulated data with application to exploring crop population structure\n2013\nAnnals of Applied Statistics\n\n\nGradient-based habitat affinities predict species vulnerability to drought\n2013\nEcology\n\n\nHIV dynamics impacting the efficacy of HIV/AIDS treatments\n2013\nJournal of Proteomics and Bioinformatics\n\n\nIn tennis, do smashes win matches?\n2013\nSignificance\n\n\nLet graphics tell the story-Datasets in R\n2013\nR Journal\n\n\nPPtree: Projection pursuit classification tree\n2013\nElectronic Journal of Statistics\n\n\nThe generalized pairs plot\n2013\nJournal of Computational and Graphical Statistics\n\n\nValidation of visual statistical inference, applied to linear models\n2013\nJournal of the American Statistical Association\n\n\nBirdsEyeView (BEV): graphical overviews of experimental data.\n2012\nBMC bioinformatics\n\n\nGlyph-maps for visually exploring temporal patterns in climate data and models\n2012\nEnvironmetrics\n\n\nGraphical tests for power comparison of competing designs\n2012\nIEEE Transactions on Visualization and Computer Graphics\n\n\nggbio: an R package for extending the grammar of graphics for genomic data\n2012\nGenome biology\n\n\ntourrGui: A gWidgets GUI for the tour to explore high-dimensional data using low-dimensional projections\n2012\nJournal of Statistical Software\n\n\nDelayed, Canceled, on Time, Boarding… Flying in the USA\n2011\nJournal of Computational and Graphical Statistics\n\n\nPKgraph: An R package for graphically diagnosing population pharmacokinetic models\n2011\nComputer Methods and Programs in Biomedicine\n\n\nPopulation structure and linkage disequilibrium in oat (Avena sativa L.): Implications for genome-wide association studies\n2011\nTheoretical and Applied Genetics\n\n\nTips for presenting your work\n2011\nR Journal\n\n\nTourr: An R package for exploring multivariate data with projections\n2011\nJournal of Statistical Software\n\n\nA projection pursuit index for large p small n data\n2010\nStatistics and Computing\n\n\nEditorial: Publishing animations, 3D visualizations, and movies in JCGS\n2010\nJournal of Computational and Graphical Statistics\n\n\nGlaciers melt as mountains warm: A graphical case study\n2010\nComputational Statistics\n\n\nGraphical inference for infovis\n2010\nIEEE Transactions on Visualization and Computer Graphics\n\n\nExtending the GGobi pipeline from R : RRapid prototyping of interactive visualizations\n2009\nComputational Statistics\n\n\nIncorporating exploratory methods using dynamic graphics into multivariate statistics classes: Curriculum development\n2009\nQuality Research in Literacy and Science Education: International Perspectives and Gold Standards\n\n\nStatistical inference for exploratory data analysis and model diagnostics\n2009\nPhilosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences\n\n\nThe plumbing of interactive graphics\n2009\nComputational Statistics\n\n\nComment\n2008\nTechnometrics\n\n\nExplorase: Multivariate exploratory analysis and visualization for systems biology\n2008\nJournal of Statistical Software\n\n\nVisual methods for examining SVM classifiers\n2008\nLecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n\n\nexploRase: Exploratory data analysis of systems biology data\n2006\nProceedings - Fourth International Conference on Coordinated and Multiple Views in Exploratory Visualization, CMV 2006\n\n\nBarleyBase - An expression profiling database for plant genomics\n2005\nNucleic Acids Research\n\n\nLimn matrix: A tool for visualizing large, distributed, high-dimension data sets\n2005\n18th International Conference on Computer Applications in Industry and Engineering 2005, CAINE 2005\n\n\nProjection pursuit for exploratory supervised classification\n2005\nJournal of Computational and Graphical Statistics\n\n\nComputational Methods for High-Dimensional Rotations in Data Visualization\n2004\nHandbook of Statistics\n\n\nA Projection Pursuit Method on the Multidimensional Squared Contingency Table\n2003\nComputational Statistics\n\n\nGGobi: Evolving from XGobi into an extensible framework for interactive data visualization\n2003\nComputational Statistics and Data Analysis\n\n\nHigh Dimensional System Design using Genetic Algorithms & Visualization\n2003\nProceedings of the American Control Conference\n\n\nMetNet: Software to build and model the biogenetic lattice of Arabidopsis\n2003\nComparative and Functional Genomics\n\n\nTowards simple, easy-to-understand, yet accurate classifiers\n2003\nProceedings - IEEE International Conference on Data Mining, ICDM\n\n\nVisualising high-dimensional data in time and space: Ideas from the Orca project\n2002\nChemometrics and Intelligent Laboratory Systems\n\n\nGaining insights into support vector machine pattern classifiers using projection-based tour methods\n2001\nProceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n\n\nVisualizing membership in multiple clusters after fuzzy c-means clustering\n2001\nProceedings of SPIE - The International Society for Optical Engineering\n\n\nInteractive visualization of hierarchical clusters using MDS and MST\n2000\nMetrika\n\n\nLinking ArcView™ and XGobi: Insight behind the Front End\n2000\nJournal of Computational and Graphical Statistics\n\n\nOrca: A visualization toolkit for high-dimensional data\n2000\nJournal of Computational and Graphical Statistics\n\n\nVisual data mining in atmospheric science data\n2000\nData Mining and Knowledge Discovery\n\n\nVisualization and case reduction in multivariate data clustering\n2000\nProceedings of SPIE - The International Society for Optical Engineering\n\n\nVisualization of data\n2000\nCurrent Opinion in Biotechnology\n\n\nVisualization of stable manifolds and multidimensional surfaces in the analysis of power system dynamics\n2000\nJournal of Nonlinear Science\n\n\nBenefits of statistical visualization in an immersive environment\n1999\nProceedings - Virtual Reality Annual International Symposium\n\n\nXGobi vs the C2: Results of an experiment comparing data visualization in a 3-D immersive virtual reality environment with a 2-D workstation display\n1999\nComputational Statistics\n\n\nExploring environmental data in a highly immersive virtual reality environment\n1998\nEnvironmental Monitoring and Assessment\n\n\nXGobi: Interactive dynamic data visualization in the X window system\n1998\nJournal of Computational and Graphical Statistics\n\n\nBinning of Kernel-based projection pursuit indices in XGobi\n1997\nComputational Statistics and Data Analysis\n\n\nCalibrate your eyes to recognize high-dimensional shapes from their low-dimensional projections\n1997\nJournal of Statistical Software\n\n\nDynamic graphics in a GIS: More examples using linked software\n1997\nComputers and Geosciences\n\n\nManual controls for high-dimensional data projections\n1997\nJournal of Computational and Graphical Statistics\n\n\nDynamic graphics in a GIS: Exploring and analyzing multivariate spatial data using linked software\n1996\nComputational Statistics\n\n\nLinked ArcView 2.1 and XGobi environment - GIS, dynamic statistical graphics, and spatial data\n1996\nProceedings of the ACM Workshop on Advances in Geographic Information Systems\n\n\nUsing Graphics and Simulation to Teach Statistical Concepts\n1996\nAmerican Statistician\n\n\nGrand tour and projection pursuit\n1995\nJournal of Computational and Graphical Statistics\n\n\nPojection pursuit indexes based on orthonormal function expansions\n1993\nJournal of Computational and Graphical Statistics\n\n\n\n\n\n\n\n\nThis list is automatically populated from orcid.org"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books\n\nCook, D. and Laa, U. (2024) Interactively exploring high-dimensional data and models in R, CRC Press.\nCook, D., and Swayne, D. (with contributions from Buja, A., Temple Lang, D., Hofmann, H., Wickham, H. and Lawrence, M.) (2007). Interactive and Dynamic Graphics for Data Analysis with examples using R and GGobi, Springer, New York. With additional data, R code and demo movies at http://www.ggobi.org."
  },
  {
    "objectID": "publications.html#refereed-conference-proceedings",
    "href": "publications.html#refereed-conference-proceedings",
    "title": "Publications",
    "section": "Refereed conference proceedings",
    "text": "Refereed conference proceedings"
  },
  {
    "objectID": "publications.html#media-videos",
    "href": "publications.html#media-videos",
    "title": "Publications",
    "section": "Media & videos",
    "text": "Media & videos\n\nMajure, J. J., Cook, D., Symanzik, J., and Megretskaia, I. (1996). An Interactive Environment for the Graphical Analysis of Spatial Data. ASA Statistical Graphics Video Lending Library.\nSymanzik, S. Cook, D., Kohlmeyer, B. D., Lechner, U. and Cruz-Neira, C. (1996) Dynamic Statistical Graphics in a Highly Immersive Environment ASA Statistical Graphics Video Lending Library.\nMorton, S., Cook, D., Stuetzle, W., and Buja, A. (1995). Computer Graphics in Statistics: The Last 30 Years in Brief. ASA Statistical Graphics Video Lending Library.\nMajure, J., Cook, D., Cressie, N., Kaiser, M., Lahiri, S., and Symanzik, J. (1995). Spatial CDF Estimation and Visualization with Applications to Forest Health Monitoring. ASA Statistical Graphics Video Lending Library.\nSymanzik, J., Majure, J. J., and Cook, D. (1995). Dynamic Graphics in a GIS: Analyzing and Exploring Multivariate Spatial Data. ASA Statistical Graphics Video Lending Library.\nMcDougall, A. and Cook, D. (1994). Exploring Time Series Using Interactive Graphics. ASA Statistical Graphics Video Lending Library\nCook, D., Buja, A., Cabrera, J., and Swayne, D. (1993). Grand Tour and Projection Pursuit. ASA Statistical Graphics Video Lending Library.\nSwayne, D. F., Cook, D. and Buja, A. (1991) XGobi: Dynamic Graphics for Data Analysis, ASA Statistical Graphics Video Lending Library."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Di Cook",
    "section": "",
    "text": "Dianne Cook is a Professor of Statistics in Econometrics and Business Statistics at Monash University in Melbourne, Australia. She has a PhD in Statistics from Rutgers University. Her research focuses on statistical graphics, which involves interactive visualisation of high-dimensional data, and statistical inference for data visualisation. In her role at Monash University she regularly teaches courses on machine learning and data analysis, and she has conducted workshops on data visualisation.\nDi is a Fellow of the American Statistical Association, past editor of the Journal of Computational and Graphical Statistics, and The R Journal, Member of the R Foundation, and elected member of the International Statistical Institute, and author of numerous R packages. She is active in R Ladies Melbourne, the Statistical Computing and Visualisation Section of the Statistical Society of Australia, and the Graphics and Computing Sections of the American Statistical Association.\nIn 2015, Di moved home to Australia after many years at Iowa State University. This allowed her to be close to family. As part of the move, she accepted emeritus status in order to continue supervising current students. She is far from retired, though, as you will see from publications, software, talks and student graduations.\nDi is passionate about open source software, ethics and transparency, and achieving gender parity in society. She is also passionate about sports and the environment. Before becoming an academic she played sports competitively, with a highlight being the opening batswoman on Bob Hawke’s Prime Minister’s XI women’s cricket team against England in 1985, played an Manuka Oval in Canberra. The environment in Australia has changed dramatically in the years since colonisation with many species becoming endangered or even extinct. For this reason, many of the R packages originating from the NUMBATs group at Monash, of which Di is a member, are based on Australian wild life, many cute and wonderful creatures. And also, data sets compiled for teaching include many from the environment, and women’s sports statistics, along with details on how it was obtained."
  },
  {
    "objectID": "posts/2014-11-03-nullabor/index.html",
    "href": "posts/2014-11-03-nullabor/index.html",
    "title": "New version of nullabor package released",
    "section": "",
    "text": "The new version of nullabor contains numerical measures that quantify how close the plot of the data is to the null plots in a lineup. It is very difficult to quantify all patterns that might be read from plots, so these should be taken in a spirit ofa Herculean task. The goal is to get some sense of what people are reacting to in a plot, which could be then associated with the text descriptions from people, or with data from an eyetracker.\nTo illustrate the use of the package, for doing inference using graphics, we adapt an example fro elementary statistics on the color distribution in packs of skittles^TM. Skittles come in different flavored bags - we will use the Wildberry packs, which have purple, red, blue, pink, and green. According to http://www.ask.com.\n“In a batch of candies produced in the Skittles factory, there are equal amounts of the five colors all mixed together before being packaged into separate bags. Consequently, each color should represent approximately 20 percent of any one skittles bag.”\nIn 2.17oz bags there are supposed to be 56 skittles on average, give or take some deformities. We can use R to simulate the distribution of colors in a bag of skittles, assuming each color is packed with equal proportion, 20%, and use Alyssa Frazee’s RSkittleBrewer package to color the plot of the data to match the candy colors.\n\n\n\n“skittles bag”\n\n\nThere were 15, 9, 10, 9, 13 purple, red, blue, pink, green skittles, with proportions 0.27, 0.16, 0.18, 0.16, 0.23 respectively. If you were to just look at this distribution, particularly if you don’t like the purple color, you might be tempted to think the Mars company is conspiring against you to put more purple skittles into the packs.\nIn an early statistics class, this might be tested using a hypothesis test:\nHo: p1=p2=p3=p4=p5 vs Ha: Not all equal\nby calculating a X^2 test statistic with expected counts for each color equal to 11.2:\nX^2=2.571, df=4, p-value=0.63. This is a large p-value, so you would conclude that there is no evidence to reject Ho, and you would conclude that the packs are actually produced using an equal proportion of all colors.\nThis requires that the audience knows a little statistics, and understands the logic behind hypothesis testing. If we were to examine the problem using the lineup protocol from visual inference, we would generate another 19 data sets from the null distribution and plot these along with the plot of the data. You would ask which of the 20 plots is the most different from the others (a little more general than asking specifically about the purple count, which enables more general discoveries of unexpected patterns).\n\n\n\n“skittles lineup 1”\n\n\nYour bag is in position 19. There is a lot of variation from one plot to another, which happens when sampling from a uniform distribution. It is not unusual to obtain 15 of one color in a pack. Sampling does not yield a flat distribution each time, numbers of each category can vary wildly in packs of size 56.\nIf you are not convinced, here is a new example. Technically, you should not view a lineup of your plotted data, after you have already seen your data. It will always stand out because you recognize it. This time, you have not been told which bag was yours, so you are asked to find the plot among the 20 which has the strangest distribution, as a way to find your bag. We’ll tell you which is yours after you pick, but the answer is at the bottom of this page.\n\n\n\n“skittles lineup 2”\n\n\nAmong the bag counts, there are bags with as few as 1 purple skittle, and as many as 19. There is a bag with 19 red skittles, and yet another with 19 pink skittles, and oh, there is a bag with 20 blue skittles. The numbers of each color vary wildly from bag to bag.\nLet’s switch up the scenario. Suppose you really are right, that the Mars company is filling bags with twice as many purple skittles as any other color. What would this look like? Here’s a lineup of your bag of skittles, where the company has filled it with a higher proportion of purple, compared with counts for bags filled using equal numbers. Pick the plot that is the most different.\n\n\n\n“skittles lineup 3”\n\n\nIf we were to conduct the hypothesis test for this data, the details would be:\nX^2=19, df=4, p-value=0.0008\nwhich would result in rejecting the null hypothesis and concluding that Mars was filling bags with too many purples. The actual proportions used to generate the data were 50% purple, and 12.5% for the other colors. The resulting samples look quite different in the plots, from those that were generated under the assumption that all colors are packed equally.\nUsing the lineup protocol is similar to the Sesame Street task, “one of these things is not like the other”. It enables illustrating statistical significance or lack of it, with less technical detail than the traditional hypothesis testing requires, and opens the door to many more problems where assumptions underlying formal hypothesis testing are not satisfied.\nThe second paragraph from http://www.ask.com provides more details on sampling variability:\n“However, due to occasional errors in the mixing process, each small sample of the overall batch will likely have deviations from the 20 percent that each color should represent. In a perfect world, the candy batch would be completely uniform before packaging, resulting in the perfect proportion of Skittles for each bag. Because errors result in minor deviations, Skittles’ consumers can expect to have anywhere from a 15 to 23 percent composition of any color when they open their bag.”\nThe nullabor package makes it easy to generate lineups. Details on the package can be found here.\nThe position of the actual data plot in the latter two lineups were 17 and 7, respectively."
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "",
    "text": "Susan Wei kicked off NUMBATs for 2025 last Thursday, with a talk about a recently completed project documented in this arxiv paper. The work is motivated by Toy Models of Superposition paper which provides an explanation of hidden layer behaviour of neural networks in the presence of differences induced by sample size, dimension and correlation. Susan’s work relates this to optimisation curiosities and particularly the connection between learning via SGD and Bayesian learning. The example used to illustrate the optimisation quirk is also an interesting from the perspective of high-dimensional visualisation using tours."
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#motivation",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#motivation",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "",
    "text": "Susan Wei kicked off NUMBATs for 2025 last Thursday, with a talk about a recently completed project documented in this arxiv paper. The work is motivated by Toy Models of Superposition paper which provides an explanation of hidden layer behaviour of neural networks in the presence of differences induced by sample size, dimension and correlation. Susan’s work relates this to optimisation curiosities and particularly the connection between learning via SGD and Bayesian learning. The example used to illustrate the optimisation quirk is also an interesting from the perspective of high-dimensional visualisation using tours."
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#about-the-data-example",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#about-the-data-example",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "About the data example",
    "text": "About the data example\nThis 6D data from Susan’s paper approach is generated by random uniform sampling along each of the six axes. The code below can generate these sort of samples.\n\n\nCode\nlibrary(tourr)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\ngenerate_toy_dataset &lt;- function(num_samples, n, noise_std, seed) {\n  set.seed(seed)\n  \n  # Generate random integers for 'a' (equivalent to JAX's randint)\n  a &lt;- sample(0:(n-1), num_samples, replace = TRUE)\n  \n  # Generate uniform random values for 'lambda_val'\n  lambda_val &lt;- runif(num_samples)\n  \n  # Create input matrix X\n  x &lt;- matrix(0, nrow = num_samples, ncol = n)\n  for (i in 1:num_samples) {\n    x[i, a[i] + 1] &lt;- lambda_val[i]  # +1 since R uses 1-based indexing\n  }\n  \n  # Generate Gaussian noise\n  gaussian_noise &lt;- matrix(rnorm(num_samples * n, mean = 0, sd = noise_std), \n                           nrow = num_samples, ncol = n)\n  \n  # Compute y\n  y &lt;- x + gaussian_noise\n  \n  return(list(x = x, y = y))\n}\n\n\nThis can be viewed using a grand tour made using the tourr package with the code below:\n\n\nCode\nset.seed(426)\nstart_proj &lt;- basis_random(6, 2)\nmyseed &lt;- 920\nn &lt;- 50\nd50 &lt;- generate_toy_dataset(n, 6, 0.05, myseed)\ndx50 &lt;- as_tibble(d50$x)\ndy50 &lt;- as_tibble(d50$y)\ntp &lt;- save_history(dx50, start = start_proj)\nrender_gif(dx50, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", center = FALSE),\n           gif_file = \"n50.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\nn &lt;- 100\nd100 &lt;- generate_toy_dataset(n, 6, 0.05, myseed)\ndx100 &lt;- as_tibble(d100$x)\ndy100 &lt;- as_tibble(d100$y)\nrender_gif(dx100, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", center = FALSE),\n           gif_file = \"n100.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\nn &lt;- 500\nd500 &lt;- generate_toy_dataset(n, 6, 0.05, myseed)\ndx500 &lt;- as_tibble(d500$x)\ndy500 &lt;- as_tibble(d500$y)\nrender_gif(dx500, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", center = FALSE),\n           gif_file = \"n500.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\nsave(dx50, file=\"d50.rda\")\nsave(dx100, file=\"d100.rda\")\nsave(dx500, file=\"d500.rda\")\nsave(dy50, file=\"dy50.rda\")\nsave(dy100, file=\"dy100.rda\")\nsave(dy500, file=\"dy500.rda\")\n\n\nAnd here are tours of the data containing different sample sizes, n=50, 100, 500, and shown as orange points. The axes producing each 2D projection in the stream of projections are represented by the line segments and circle. You can see that this data only lies along an axis. It can be interpreted as if one variable has some non-zero value, all other values are zero. It’s a contrived example, but it does arise from analysis of large text data, where each word is unique so when used the other words in the set are not used. This is the data that generates the interesting phase shifts during optimisation.\n\n\nn=50\n\n\nn=100\n\n\nn=500"
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#k-gons",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#k-gons",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "\\(k\\)-gons",
    "text": "\\(k\\)-gons\nHere we connect the most extreme point on each axis, and tour again. Because there are 6 axes, and we connect each extreme to every other extremes there are 15 edges in total. By focusing only the convex hull of these for each projection you can see that different polygon (\\(k\\)-gon) shapes are seen in the 2D projections. The number of vertices visible will range from 3-6, so we have 3-, 4-, 5- and 6-gons. A 6-gon is formed when all six axes extend radially from the centre. It would be an ideal 6-gon (hexagon) if the angles between axes were equal, and the length of the line segments were equal.\n\n\nCode\nload(\"d50.rda\")\nload(\"d100.rda\")\nload(\"d500.rda\")\nmx50 &lt;- c(which.max(dx50$V1),\n        which.max(dx50$V2),\n        which.max(dx50$V3),\n        which.max(dx50$V4),\n        which.max(dx50$V5),\n        which.max(dx50$V6))\nedges50 &lt;- NULL\nfor (i in 1:5) \n  for (j in (i+1):6) \n    edges50 &lt;- rbind(edges50, c(mx50[i], mx50[j]))\ncolnames(edges50) &lt;- c(\"from\", \"to\")\nmx100 &lt;- c(which.max(dx100$V1),\n        which.max(dx100$V2),\n        which.max(dx100$V3),\n        which.max(dx100$V4),\n        which.max(dx100$V5),\n        which.max(dx100$V6))\nedges100 &lt;- NULL\nfor (i in 1:5) \n  for (j in (i+1):6) \n    edges100 &lt;- rbind(edges100, c(mx100[i], mx100[j]))\ncolnames(edges100) &lt;- c(\"from\", \"to\")\nmx500 &lt;- c(which.max(dx500$V1),\n        which.max(dx500$V2),\n        which.max(dx500$V3),\n        which.max(dx500$V4),\n        which.max(dx500$V5),\n        which.max(dx500$V6))\nedges500 &lt;- NULL\nfor (i in 1:5) \n  for (j in (i+1):6) \n    edges500 &lt;- rbind(edges500, c(mx500[i], mx500[j]))\ncolnames(edges500) &lt;- c(\"from\", \"to\")\n\n\n\n\nCode\nanimate_xy(dx500, \n           center=FALSE, \n           col = \"orange\",\n           edges = edges500,\n           edges.col = \"orange\")\nrender_gif(dx50, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", \n                                center = FALSE,\n                                edges = edges50,\n                                edges.col = \"orange\"),\n           gif_file = \"n50_gon.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\nrender_gif(dx100, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", \n                                center = FALSE,\n                                edges = edges100,\n                                edges.col = \"orange\"),\n           gif_file = \"n100_gon.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\nrender_gif(dx500, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", \n                                center = FALSE,\n                                edges = edges500,\n                                edges.col = \"orange\"),\n           gif_file = \"n500_gon.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)"
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#the-optimisation-problem",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#the-optimisation-problem",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "The optimisation problem",
    "text": "The optimisation problem\nIn Susan’s example, the optimisation is training a neural network designed to take 6D data transform it into 2D and then recover the 6D data again. The model is learning the coefficients for the 2D projection, that is, the axes as shown in the representations from a tour. The latter is the same input data with some additional noise. I’m have no idea why this is an interesting problem, except perhaps related to finding a useful 2D representation of the data structure, one that would give the viewer a good chance of recognising the underlying structure of the high-dimensional data. This type of neural network training is the basis of the Toy Models of Superposition paper.\n\n\nCode\nload(\"dy500.rda\")\nrender_gif(dy500, \n           tour_path = planned_tour(tp),\n           display = display_xy(col = \"orange\", \n                                center = FALSE),\n           gif_file = \"ny500.gif\",\n           width = 400,\n           height = 400,\n           frames = 500,\n           loop = TRUE)\n\n\n\n\n\nINPUT\n\n\n\n\nHIDDEN LAYER\n\n\n\n\n\n\n\n\n\n\n\nOptimal configuration?\n\n\n3, 4, 5 or 6?\n\n\n\nOUTPUT"
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#optimisation-curiosity",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#optimisation-curiosity",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "Optimisation curiosity",
    "text": "Optimisation curiosity\nThis is the illustration of the optimisation, from Susan’s paper. The loss appears to have almost discrete steps with big decreases between \\(k\\)-gons. The vertices of polygons (in the convex hull, at least) are considered to be critical points in the 2D projection reached at the different phases of the optimisation."
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#random-projections-of-the-data",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#random-projections-of-the-data",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "Random projections of the data",
    "text": "Random projections of the data\nThe optimisation is working through the space of all possible 2D projections of the data. Here is a sample of 8 random projections. This is a (tiny) sample of the space that the optimisation is working from.\n\n\nCode\nlibrary(patchwork)\nset.seed(1107)\nprj &lt;- basis_random(6, 2)\np1 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np2 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np3 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np4 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np5 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np6 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np7 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nprj &lt;- basis_random(6, 2)\np8 &lt;- render_proj(dx500, prj)$data_prj |&gt;\n  ggplot() + \n  geom_point(aes(x=P1, y=P2),\n               colour = \"orange\") +\n      xlim(-1,1) + ylim(-1, 1) +\n      theme_bw() +\n      theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + plot_layout(ncol=4)"
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#about-the-phase-shifts",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#about-the-phase-shifts",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "About the phase shifts",
    "text": "About the phase shifts\nThe phase shifts are likely or should be happening at the different perfect shapes, which are special projections of the 6D data:\n\n\nCode\nload(\"d50.rda\")\nload(\"d100.rda\")\nload(\"d500.rda\")\np_trigon &lt;- matrix(c(1, 0, \n                     cos(pi*2/3), sin(pi*2/3), \n                     cos(-pi*2/3), sin(-pi*2/3),\n                     0, 0,\n                     0, 0, \n                     0, 0), ncol=2, byrow=TRUE)\np_trigon &lt;- orthonormalise(p_trigon)\ndx500_p &lt;- render_proj(dx500, p_trigon)\ndx500_p_edges &lt;- cbind(\n  rbind(dx500_p$data_prj[mx500[1],1:2], \n  dx500_p$data_prj[mx500[2],1:2],\n  dx500_p$data_prj[mx500[3],1:2]),\n  rbind(dx500_p$data_prj[mx500[2],1:2], \n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[1],1:2]))\ncolnames(dx500_p_edges) &lt;- c(\"x_from\", \"y_from\", \"x_to\", \"y_to\")\nplt_tri &lt;- ggplot() +\n    geom_point(data=dx500_p$data_prj, aes(x=P1, y=P2),\n               colour = \"orange\") +\n    geom_path(data=dx500_p$circle, aes(x=c1, y=c2)) +\n    geom_segment(data=dx500_p$axes, aes(x=x1, y=y1, xend=x2, yend=y2)) +\n    geom_text(data=dx500_p$axes, aes(x=x2, y=y2, label=rownames(dx500_p$axes))) +\n    geom_segment(data=dx500_p_edges, \n                 aes(x=x_from, y=y_from,\n                     xend=x_to, yend=y_to),\n               colour = \"orange\") +\n    xlim(-1,1) + ylim(-1, 1) +\n    theme_bw() +\n    theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\np_4gon &lt;- matrix(c(1, 0, \n                     cos(pi*2/4), sin(pi*2/4), \n                     cos(2*pi*2/4), sin(2*pi*2/4), \n                     cos(-pi*2/4), sin(-pi*2/4),\n                     0, 0, \n                     0, 0), ncol=2, byrow=TRUE)\np_4gon &lt;- orthonormalise(p_4gon)\ndx500_p &lt;- render_proj(dx500, p_4gon)\ndx500_p_edges &lt;- cbind(\n  rbind(dx500_p$data_prj[mx500[1],1:2], \n  dx500_p$data_prj[mx500[2],1:2],\n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2]),\n  rbind(dx500_p$data_prj[mx500[2],1:2], \n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2],\n  dx500_p$data_prj[mx500[1],1:2]))\ncolnames(dx500_p_edges) &lt;- c(\"x_from\", \"y_from\", \"x_to\", \"y_to\")\nplt_quad &lt;- ggplot() +\n    geom_point(data=dx500_p$data_prj, aes(x=P1, y=P2),\n               colour = \"orange\") +\n    geom_path(data=dx500_p$circle, aes(x=c1, y=c2)) +\n    geom_segment(data=dx500_p$axes, aes(x=x1, y=y1, xend=x2, yend=y2)) +\n    geom_text(data=dx500_p$axes, aes(x=x2, y=y2, label=rownames(dx500_p$axes))) +\n    geom_segment(data=dx500_p_edges, \n                 aes(x=x_from, y=y_from,\n                     xend=x_to, yend=y_to),\n               colour = \"orange\") +\n    xlim(-1,1) + ylim(-1, 1) +\n    theme_bw() +\n    theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\np_5gon &lt;- matrix(c(1, 0, \n                     cos(pi*2/5), sin(pi*2/5), \n                     cos(2*pi*2/5), sin(2*pi*2/5), \n                     cos(3*pi*2/5), sin(3*pi*2/5), \n                     cos(-pi*2/5), sin(-pi*2/5),\n                     0, 0), ncol=2, byrow=TRUE)\np_5gon &lt;- orthonormalise(p_5gon)\ndx500_p &lt;- render_proj(dx500, p_5gon)\ndx500_p_edges &lt;- cbind(\n  rbind(dx500_p$data_prj[mx500[1],1:2], \n  dx500_p$data_prj[mx500[2],1:2],\n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2],\n  dx500_p$data_prj[mx500[5],1:2]),\n  rbind(dx500_p$data_prj[mx500[2],1:2], \n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2],\n  dx500_p$data_prj[mx500[5],1:2],\n  dx500_p$data_prj[mx500[1],1:2]))\ncolnames(dx500_p_edges) &lt;- c(\"x_from\", \"y_from\", \"x_to\", \"y_to\")\nplt_pent &lt;- ggplot() +\n    geom_point(data=dx500_p$data_prj, aes(x=P1, y=P2),\n               colour = \"orange\") +\n    geom_path(data=dx500_p$circle, aes(x=c1, y=c2)) +\n    geom_segment(data=dx500_p$axes, aes(x=x1, y=y1, xend=x2, yend=y2)) +\n    geom_text(data=dx500_p$axes, aes(x=x2, y=y2, label=rownames(dx500_p$axes))) +\n    geom_segment(data=dx500_p_edges, \n                 aes(x=x_from, y=y_from,\n                     xend=x_to, yend=y_to),\n               colour = \"orange\") +\n    xlim(-1,1) + ylim(-1, 1) +\n    theme_bw() +\n    theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\np_6gon &lt;- matrix(c(1, 0, \n                     cos(pi*2/6), sin(pi*2/6), \n                     cos(2*pi*2/6), sin(2*pi*2/6), \n                     cos(3*pi*2/6), sin(3*pi*2/6), \n                     cos(4*pi*2/6), sin(4*pi*2/6), \n                     cos(-pi*2/6), sin(-pi*2/6)), ncol=2, byrow=TRUE)\np_6gon &lt;- orthonormalise(p_6gon)\ndx500_p &lt;- render_proj(dx500, p_6gon)\ndx500_p_edges &lt;- cbind(\n  rbind(dx500_p$data_prj[mx500[1],1:2], \n  dx500_p$data_prj[mx500[2],1:2],\n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2],\n  dx500_p$data_prj[mx500[5],1:2],\n  dx500_p$data_prj[mx500[6],1:2]),\n  rbind(dx500_p$data_prj[mx500[2],1:2], \n  dx500_p$data_prj[mx500[3],1:2],\n  dx500_p$data_prj[mx500[4],1:2],\n  dx500_p$data_prj[mx500[5],1:2],\n  dx500_p$data_prj[mx500[6],1:2],\n  dx500_p$data_prj[mx500[1],1:2]))\ncolnames(dx500_p_edges) &lt;- c(\"x_from\", \"y_from\", \"x_to\", \"y_to\")\nplt_hex &lt;- ggplot() +\n    geom_point(data=dx500_p$data_prj, aes(x=P1, y=P2),\n               colour = \"orange\") +\n    geom_path(data=dx500_p$circle, aes(x=c1, y=c2)) +\n    geom_segment(data=dx500_p$axes, aes(x=x1, y=y1, xend=x2, yend=y2)) +\n    geom_text(data=dx500_p$axes, aes(x=x2, y=y2, label=rownames(dx500_p$axes))) +\n    geom_segment(data=dx500_p_edges, \n                 aes(x=x_from, y=y_from,\n                     xend=x_to, yend=y_to),\n               colour = \"orange\") +\n    xlim(-1,1) + ylim(-1, 1) +\n    theme_bw() +\n    theme(aspect.ratio=1,\n       axis.text=element_blank(),\n       axis.title=element_blank(),\n       axis.ticks=element_blank(),\n       panel.grid=element_blank())\nplt_tri + plt_quad + plt_pent + plt_hex + plot_layout(ncol=4)\n\n\n\n\n\n\n\n\n\nThese are perfect 3-, 4-, 5-, 6-gon shapes, and the transition to a smaller loss should happen when the extra axis gets drawn out from 0. As you have seen there are many in-between shapes, generated by small changes in the projection basis, producing polygons with 3, 4, 5, or 6 vertices which are not regularly placed."
  },
  {
    "objectID": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#next",
    "href": "posts/2025-03-11-phase-shifts-in-stochastic-gradient-descent/index.html#next",
    "title": "Phase transitions in Stochastic Gradient Descent, from a High-dimensional Data Visualisation Perspective",
    "section": "Next",
    "text": "Next\nStay tuned for the next blog post using high-dimensional visualisation that teases apart what the Toy Models of Superposition paper is trying to illustrate."
  },
  {
    "objectID": "posts/2015-11-06-cornell-statistics/index.html",
    "href": "posts/2015-11-06-cornell-statistics/index.html",
    "title": "Statistical Sciences, Cornell University",
    "section": "",
    "text": "This week I have been visiting the Department of Statistical Sciences at Cornell University. This is the home of many venerable statisticians. At first sight it appears that statisticians are spread all over the university, and technically they are because funding comes from many directions, but almost all are actually located in a suite in Comstock Hall. Professor Paul Velleman is one of the pioneers of data-centrist thinking about statistics. He produced the software called DataDesk in the early 90s that some saw as rivaling LispStat and particularly JMP for introductory statistics classes. It has model fitting and interactive graphics, that is still not available in R. It is not open source, though. His textbook for introductory statistics is one of the pre-dominant texts used globally.\nIts been an interesting week. Professor Marty Wells has been comparing results from collecting data using Amazon’s Mechanical Turk with traditional telephone surveys. Professor Jim Booth, chair of Biological Statistics and Computational Biology, has worked in many areas, most currently focused on high-throughput data, and discussed Gordon Smyth’s new work on VOOM. Professor Giles Hooker, a fellow Australian, has some recent inference results for random forests, which might be useful in my student Natalia Da Silva’s thesis research. I had several detailed discussions with Professor Jacob Jien on R packages and optimization, and using Hadley’s development tools and Yihui’s documenting. Professor Stanislav Volgushev suggested that we could build models from crowdsourcing by having people sketch, which might do better than smoothing and logistic regression sometimes! Professors Marten Wegkamp and and Flori Bunea are colleagues from a 2008 semester long workshop at Cambridge on high-dimensional problems in Statistics.\nLook out for Amy Willis! She will graduate in about a year, after working on statistics for high-throughput data collected on microbial biomes. She has an R package called breakaway.\nHere is a link to my slides on Statistical Inference by Crowd-sourcing."
  },
  {
    "objectID": "posts/2019-09-13-touring-multicollinearity/index.html",
    "href": "posts/2019-09-13-touring-multicollinearity/index.html",
    "title": "How to use a tour to check if your model suffers from multicollinearity",
    "section": "",
    "text": "This was one of the comments from a recent review of a paper:\n\nAs you note in the paper, it seems likely that there are still issues with multi-collinearity\n\nMulticollinearity means that the observations are co-linear in some combination of the variables. This has been relaxed in practice to mean substantial association between explanatory variables. When your explanatory variables have substantial association between them, it means that you don’t have a stable base on which to build a model. I like to say its like building a table with only two legs, or three legs but they are all in a row. The table top is going to be very wobbly. You need to have the legs placed broadly to provide a good base for the table top. Its similar for model fitting. You can think of the model like the table top, and the explanatory variables form the base. If the observations are not well spread out in the space, then the model fit will be unstable.\nWhen there are only two predictors (explanatory variables) its easy to check, by plotting the two variables. With more variables its harder to detect. Common approaches are to use principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or most) of the variation is explained with a subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.\nAnother approach is to compute the variance inflation factor for all the variables in your model. This measures how much the coefficients for the variable change when other variables are included with it in the model. This is the stability mentioned earlier - if there is multicollinearity the model coefficient for a variable can change substantially depending on whether another variable (or more) is included in the model or not. High variance inflation factors means that this variable’s coefficient changes a lot depending on the other variables, indicating multicollinearity among the collection of explanatory variables."
  },
  {
    "objectID": "posts/2019-09-13-touring-multicollinearity/index.html#multicollinearity",
    "href": "posts/2019-09-13-touring-multicollinearity/index.html#multicollinearity",
    "title": "How to use a tour to check if your model suffers from multicollinearity",
    "section": "",
    "text": "This was one of the comments from a recent review of a paper:\n\nAs you note in the paper, it seems likely that there are still issues with multi-collinearity\n\nMulticollinearity means that the observations are co-linear in some combination of the variables. This has been relaxed in practice to mean substantial association between explanatory variables. When your explanatory variables have substantial association between them, it means that you don’t have a stable base on which to build a model. I like to say its like building a table with only two legs, or three legs but they are all in a row. The table top is going to be very wobbly. You need to have the legs placed broadly to provide a good base for the table top. Its similar for model fitting. You can think of the model like the table top, and the explanatory variables form the base. If the observations are not well spread out in the space, then the model fit will be unstable.\nWhen there are only two predictors (explanatory variables) its easy to check, by plotting the two variables. With more variables its harder to detect. Common approaches are to use principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or most) of the variation is explained with a subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.\nAnother approach is to compute the variance inflation factor for all the variables in your model. This measures how much the coefficients for the variable change when other variables are included with it in the model. This is the stability mentioned earlier - if there is multicollinearity the model coefficient for a variable can change substantially depending on whether another variable (or more) is included in the model or not. High variance inflation factors means that this variable’s coefficient changes a lot depending on the other variables, indicating multicollinearity among the collection of explanatory variables."
  },
  {
    "objectID": "posts/2019-09-13-touring-multicollinearity/index.html#tour",
    "href": "posts/2019-09-13-touring-multicollinearity/index.html#tour",
    "title": "How to use a tour to check if your model suffers from multicollinearity",
    "section": "Tour",
    "text": "Tour\nThis is my favorite method for checking for multicollinearity. I usually do the routine approaches first (PCA and VIFs), and then run a tour. Here is the example from the paper, Jeremy Forbes, Rob Hyndman and myself are currently working on. There were actually multiple models fit, and I’ll show just two, because they do show the range.\n\nlibrary(tourr)\nquartz()\nload(\"data/d1.rda\")\nload(\"data/d2.rda\")\nanimate_xy(d1, axes=\"bottomleft\", half_range=1.2)\nrender_gif(d1, grand_tour(), display_xy(, \n           axes=\"bottomleft\", half_range=1.2), \n           frames = 100, \n           gif_file=\"d1.gif\")\nrender_gif(d2, grand_tour(), display_xy(, \n           axes=\"bottomleft\", half_range=1.2), \n           frames = 100, \n           gif_file=\"d2.gif\")\n\nThis is the first set of explanatory variables. There are 150 observations and 32 variables. That’s about 5 observations for each dimension – its sparse.\n(If you are new to tours, a good resource is to look at the Wickham et al (2011) as cited in the tourr package.)\n\nModel 1\n\nThere is some collinearity, but there is also a little more. There is a concentration of points in the first couple of variables, which likely corresponds to a lot of very small values on these variables. It can be hard to fix this, usually a log or power transformation would be used. There are also a few outliers, just a few, and they are not very extreme. These are the points that float out from the others occasionally. The last frame below shows a point where the collinearity is visible. The points flatten into a tall pancake, which says that there is moderate association between them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2\n\nThe second example is similar to the first. There is also some nonlinear association visible, but the outliers are less extreme."
  },
  {
    "objectID": "posts/2019-09-13-touring-multicollinearity/index.html#summary",
    "href": "posts/2019-09-13-touring-multicollinearity/index.html#summary",
    "title": "How to use a tour to check if your model suffers from multicollinearity",
    "section": "Summary",
    "text": "Summary\nWhile there is evidence of multicollinearity for both of these sets of explanatory variables, its not very severe. Its more like a table built with one thick central plank for support. These sort of tables exist and are reasonably stable. We can not be absolutely certain, but the conclusion would be that the models are robust.\nAn aside about multicollinearity and model fitting purpose: we typically consider this to be a problem only if the purpose is explanatory rather than predictive. If the goal is to predict from the model and the new predictor values are in the range of the existing data, then the predictions are ok. Think about it like the table on two legs, the line at which the table top sits on the legs is the same, even if the table top pivots around this line. For explanation, because the coefficients can change dramatically depending on the other variables in the model, the interpretation of the coefficient is unreliable with multicollinearity.\nNow, I dare someone to add a sentence to their next model application paper like “Multicollinearity was checked using a tour on all the explanatory variables, run for 3.46 minutes.” (Ha, ha)"
  },
  {
    "objectID": "posts/2019-09-13-touring-multicollinearity/index.html#sources",
    "href": "posts/2019-09-13-touring-multicollinearity/index.html#sources",
    "title": "How to use a tour to check if your model suffers from multicollinearity",
    "section": "Sources",
    "text": "Sources\nThe code for this post is here."
  },
  {
    "objectID": "posts/2014-09-28-kygios/index.html",
    "href": "posts/2014-09-28-kygios/index.html",
    "title": "How good is Nick Kygrios?",
    "section": "",
    "text": "Nick Kygrios caught the world’s attention in July at Wimbledon 2014 when he beat world number 1 Rafael Nadal. After the match McEnroe commented:\n“We’ve been waiting for this for a while. We keep saying, `Who’s the next guy?’, and I think we found that guy right now.” http://dailym.ai/1jlEfrc.\nbut Nadal seemed to beg to differ:\n“He has things, positive things, to be a good player. But everything is a little bit easier when you are arriving.”\nI scraped the data from the Wimbledon web site, for the first two rounds of the tournament to compare Nick’s play with the other competitors. Making plots of these statistics we can learn that Nick really was pushing his game, or maybe he always pushes his game, making an extremely high number of unforced errors, but balancing this with a large number of winners. His statistics on first serve, receiving points and net points were comparable to the players who made it to the semi-finals. His second serve percentage was good but on the low side compared to the semi-finalists.\nThe relationship between the statistics and how far the competitors make it through the tournament is not simple. For example you might think that the more winners a player hits the better the chance that they win a match. This isn’t true, and a wonderful example is Bethanie Mattek-Sands, who hit 81 winners in her 2012 Australian Open first round match, 30 more than any other player, and lost the match. It wasn’t her that unforced errors were disproportionately high either, they were on the right side of the winners at 65. Players typically need to keep their statistics in a range, not too high and not too low, with a few exceptions. Hitting winners is important, but only up to a point, and it helps if there are more winners than unforced errors. Simply modeling the statistics against progression through the tournament doesn’t work well. Making plots we can see the relationships, but we can also imagine relationships when they don’t actually exist.\nMaking plots of the data in the context of plots where there is no relationship, using permutations, can help assess the importance of the relationship. See Chance column for more information on how this was done for Nick Kygrios’ statistics.\nSince that article was written we have had the luxury of seeing Nick in his next grand slam tournament, the 2014 US Open. He had a tough draw, Youzhny in the first round, Seppi in the second, and Robredo in the third. His statistics were up there again, comparable with the best. But his main problem right now is the wildness: against Robredo, as it happened against Raonic in Wimbledon, playing against a steady opponent willing to wait for their chances to strike back, toppled his game. He has the physical game, but his mental game is not at its necessary level to be a champion yet."
  },
  {
    "objectID": "posts/2014-11-16-eda-at-the-un/index.html",
    "href": "posts/2014-11-16-eda-at-the-un/index.html",
    "title": "EDA at the UN",
    "section": "",
    "text": "On Nov 10 I was part of a celebration of John W. Tukey at the United Nations. This event kicked off a new UN initiative called Unite Ideas. Details of the event, and the initiative can be found here. There were five talks relayed live to an audience of several thousand, using google hangouts and a youtube channel, and listeners could post questions using the Q/A tool.\nMy talk was titled “An Exploratory Data Analysis of OECD’s 2012 PISA Survey”s and I delivered it by computer from my office in Iowa. The experience of talking to the computer without visual feedback of the audience felt awkward. My parents in Australia got up at 1am their time to tune in, and they reported “went ok”, and “we liked how you answered the questions”. I decided to talk on this topic because it showcased EDA tools for exploring data rather than talking about tools. I worked with Ian Lyttle, Alex Shum and Luke Fostveldt exploring the data over the summer as part of a competition for useR! 2014, and have previously talked about it on this in our working group. The education data is huge, and I was very surprised by what we found, and that is what EDA can do “the greatest value of a picture is when it forces us to notice what we never expected to see” (quote by Tukey). Often what I find in data are just problems, good to know about, but a little embarrassing to the data owner. Plots of this data blew up one of the myths that I was living with.\nThere were a few questions posted, and this is the main reason for this blog. It is to address this question:\n“When you draw conclusions about response variables based on one-predictor-at-at-time analyses, you are of course quite likely to be a victim of confounding.”\nI answered this live, perhaps not very well, but I have also been asked similar questions at various times over the years, and it is a good question, worth further attention. The question arises because I looked just at differences in math scores just for one covariate, separately by country - this is what the “one predictor at a time” means. By only looking at gender, and not at school, age, hours spent studying, parents’ socioeconomic status, truancy, number of possessions, number of math teachers, attitude to math, … there is a danger that the results change at a finer resolution. Yes, this is something that you should always be cautious about.\n\n\n\ngender bias in math by country\n\n\nTo counter this, a modeler would fit many, many variables and the interactions simultaneously and look at the coefficients for the predictors in the model, in order to make a statement about gender and math scores. However, this is at odds, to a large extent with EDA, where the fundamentals dictate light processing of the data. If you overly process the data you might miss key features like outliers, clusters, small pockets of heterogeneity. My first instincts with the gender bias is not universal observation was to be skeptical: could it really be that in Qatar girls score 15 points better than boys on average? This could be an artifact induced by small sample size, or unequal proportions of gender in the sample. Simpson’s paradox can occur when proportions and counts are different between categories that are aggregated - look up the famous Berkeley admissions data.\nNeither of these are culprits for these five countries with the reverse gender gap. There were 7000 students, with 51% girls and 49% boys being measured in Jordan. In addition, students were drawn from about 250 different schools. These are quick and easy checks that can be done for data like this. Actually for many of the plots that we made of the data we also made bar charts of the category counts to guard against reading too much into the patterns of the math scores. You should be suspicious of results reported without these numbers.\nAnother check can be done, which is the lineup protocol discussed in an earlier blog. Put the plot of the data in amongst a field of plots of null data. We can create the null data sets for this data by permuting the gender labels of students, within each country, recalculate the difference in average math scores, and remake the dotplot. This is done below.\n\n\n\ngender bias lineup\n\n\nThe plot of the data (f) stands out amongst the rest of the plots. The largest gap that is produced by random labels is on the order of 5 points, and we see a pretty even spread between boys and girls. The data stands out in this bunch because there are more countries that have a higher boys average score and because some countries have much bigger average differences than 5 points. The nearest that we get to something odd is on plot (e) where there is one country where there is a gender gap in favor of girls of about 18 points. However, this is due to the one country with small numbers, Lichtenstein, which only measured 300 students. It should have been removed from the analysis ahead of time.\nAside: there was another question/comment, “When you see it… of course it is there!”, that appeared when I showed this lineup. I didn’t have a chance to answer it. What you see in a plot is typically a sample, and so you need to gauge whether the pattern observed is going to occur if you take another sample, or disappear. This is what “really there” means, that the pattern is present in the population. Deviations will occur from sample to sample, and it is important to ignore these for the most part.\nOne last comment about using single predictors. We cannot say definitively that there is not a universal gender gap in math. This would require a designed experiment, a huge one. The PISA data is observational data. It suggests that there isn’t a universal gender gap, and this stands up to a couple of quick checks. If this pattern repeated itself in other testing years then this would be further evidence, but not proof. You can’t always prove things with observational data but with enough different elements the evidence builds up. The other piece of information that I have in favor of the reverse gender gap in the middle eastern countries is that my female colleagues in statistics education at Duke University were not at all surprised. They knew about this pattern. But for me, and most of my colleagues, making a few pictures of the data shattered a myth.\nVideo footage of the event is available here."
  },
  {
    "objectID": "posts/2015-05-17-graduations/index.html",
    "href": "posts/2015-05-17-graduations/index.html",
    "title": "Graduates in Statistical Graphics Research at ISU 2015",
    "section": "",
    "text": "Its exciting to report on the graduations from the working group this year.\n\nNiladri Roy Chowdhury defended his PhD thesis in Aug 2014, titled “Explorations of the lineup protocol for visual inference: application to high dimension, low sample size problems and metrics to assess the quality”, under my direction. He is a scientist at Novartis, Boston, MA.\nSusan Vanderplas defended her PhD in May, titled “Perception in Statistical Graphics”, under the direction of Professor Heike Hofmann. Susan will be working for Nebraska power in the next couple of years, while continuing statistical research with colleagues at ISU. Personal logistics meant she had to turn down an assistant professor position at University of Iowa this year, but she hopes to move in to academia in several years, if their dual careers allow.\n\nXiaoyue (Zoe) Cheng defended her thesis titled “Interactive Visualization for Missing Values, Time Series, and Areal Data” under the direction of myself, and Heike. She will be an assistant professor at University of Nebraska-Omaha from August, and is welcoming a new member into the family in just a couple of weeks.\nKarsten Maurer will defend his PhD late this summer, and will start as an assistant professor at Miami University, Ohio in August.\nSam Helmich defended his MS in March, titled “Board Game Recommendation” under the direction of Professor Heike Hofmann. He works for John Deere, Iowa.\nSam Tyner defended her MS in April, titled “A Geometry for Network Visualization in ggplot2” under the direction of Professor Heike Hofmann. She will continue on to a PhD at ISU.\nDanny Bero defended his MS, titled “Ranking Iowa High School Soccer Teams”, in April, under my direction. You can check out the web app to explore rankings of high school teams, and predict match outcomes. He will take a position at an internet sales company in Monroe, WI, in June.\n\nCongratulations on the hard, and creative work!\nBelow are some photos from the spring barbecue celebrating the graduation, and the graduation photos. (Some of the people at the bbq: Ian, Lari, Alex, Andee, Cari, Vianey, Natalia, Ignacio, Eric, Rainie, Heike, Mattias, Grace.)"
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html",
    "href": "posts/2018-04-14-rookie-mistakes/index.html",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "",
    "text": "In this assignment, the focus was to practice data cleaning. Students suggested questions to build a class survey, to get to know the interests of other class members, and then completed the composed survey. After cleaning the data, a few summary plots of interesting aspects of the data were made. There are some common mistakes that rookies often make when constructing data plots: packing too much into a single graphic, leaving categorical variables unordered, reversing norms for response and explanatory variables, conditioning in wrong order, plotting counts when proportions should be the focus, not normalizing by counts, using a boxplot for small sample size.\nThis is an explanation of how to avoid common mistakes. We have taken example plots from different group submissions for the assignment, and show how to re-work the plot so that it is more effective at communicating the intended information. The fixes follow these basic principles:\nPlease don’t be discouraged if you see one of the plots you made in this document, you all did a great job of creating plots and these are suggestions to take your creations and make them better!"
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#reduce-complexity",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#reduce-complexity",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Reduce complexity",
    "text": "Reduce complexity\nThere is a temptation to pack as much as possible in a single plot. After all, you only get to put a small number of plots in a report. This makes it harder to read anything from the plot, and thus more difficult for the reader to learn about the data.\nIt is better to break up information dense graphics into smaller chunks. If there is a constraint on the number of plots you can put in a report, you could make an ensemble graphic containing a number of plots, which would count as a single plot. Facetting is an inbuilt way that the grammar of graphics facilitates chunking, to make an ensemble of plots.\nLet’s look at an example of an overly complex plot of the interaction between three variables - Q30, Q31, and Q17. There’s a lot going on in this plot.\n\n\n\n\n\n\n\n\n\nWe interpret the information that we want to communicate is: how enjoyment of the unit, and whether you are struggling, depends on having prior coding experience or not. The chunks are the three pairwise combinations of the three variables. This is a good start to tackling the primary purpose.\n\n\n\n\n\n\n\n\n\nFrom the three pairwise plots we learn:\n\nPrior coding experience leads to more likely enjoying the class.\nStruggling with the unit tends to lead to less enjoyment.\nPrior coding experience strongly indicates less struggling.\n\nIt may be better to also reflect the sample sizes in each group, using a mosaic plot.\n\n\n\n\n\n\n\n\n\nNow let’s put it all into one plot again. Using the mosaic plot, prior coding experience (Q17), and struggling (Q31) are woven into the horizontal axis, and enjoyment (Q30) is mapped to colour, and splits the vertical bars.\n\n\n\n\n\n\n\n\n\nWhat do we learn?\n\nThere are few people with no coding experience and are not struggling.\nThe largest group of unhappy class members have no prior coding experience, and are struggling.\nMost people who have prior coding experience and are not struggling, are enjoying the class.\nAbout a third of the students who have prior coding experience, and struggling with the class, are unhappy.\n\nRemember one purpose of plots is to communicate what you’ve found in the data to the reader - a more complex plot forces a reader to take longer to understand your findings and has a narrow viewpoint. Breaking a complex plot into chunks allows your reader to slowly gain a richer understanding of the data."
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#order",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#order",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Order",
    "text": "Order\nWhich is easier to read. This:\n\n\n\n\n\n\n\n\n\nor this:\n\n\n\n\n\n\n\n\n\nIn an unordered bar chart, the reader has to spend more time searching for which is the most frequent category, and it isn’t immediately obvious what the second or third most popular superpower is.\nBy reordering the categories by the count, this information is more readily available to the reader.\nIt is hard to re-order! And hence, taking the extra time to make this happen can be dispiriting. However, it is realy easy with the forcats package. Simply use the fct_infreq function when you set the x variable in ggplot:\naes(x=fct_infreq(Q6)), fill=variable)"
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#comparing-proportions-rather-than-counts",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#comparing-proportions-rather-than-counts",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Comparing proportions rather than counts",
    "text": "Comparing proportions rather than counts\nThe plot below was submitted to show examine if the hours studied by students differed whether they were doing a single or double degree. The expectation would be that double degree students need to spend more time studying.\n\n\n\n\n\n\n\n\n\nIt is helpful to think about which is the explanatory variable and which the response. Here it would be: degree type is explaining study hours. Therefore we need to examine study hours, conditional on degree type.\nThe plot above would be ok to answer this question, if the same total number of students were doing single vs double degrees. It almost is, but because there is a different number of students in each category, the counts for study hours for each degree type cannot be directly compared. We need to examine proportion of single degree students in each of the study times, and similarly for double degree students.\nThis would correspond to the plot below.\n\n\n\n\n\n\n\n\n\nThe problem that we have in digesting the proportions from this plot, is that only the very bottom group (0-3, more than 12) and top groups are easy to compare. The rest start and end at different positions, and hence the height of the bars difficult to compare.\nIdeally the bars for degree type, would be side-by-side for each study hour category. That’s very close to the original design, with the exception that the heights of the bars should represent proportions.\n\n\n\n\n\n\n\n\n\nWhat do we learn?\n\nBoth degree type students tend to put a lot of study time in each week.\nDouble degree students are not putting in more study time than single degree students.\n\nHere’s another plot submitted to study a similar relationship, study by year in university. The group summary was:\n“We can see overall 3rd year students put a lot more hours into study per week. This could perhaps be due to increased workload during the 3rd year as opposed to 1st year.”\n\n\n\n\n\n\n\n\n\nTo answer the original question you need to look at the distribution of hours studying, within each year.\nFacet by year in school, and then make a bar chart in each facet. You can see that most students are in year 2 or 3, and the counts increase over hours spent studying. Both years have this pattern.\n\n\n\n\n\n\n\n\n\nBecause numbers are so small in all other groups, drop them, and focus only on years 2 and 3. Then we need to do the same calculations as the previous problem, to compare proportion in each study time, within year.\n\n\n\n\n\n\n\n\n\nWhat do we learn?\n\nBoth years spend long hours studying\nMore third years report spending more than 12 hours, but its a fairly small difference in proportions. And by contrast the proportions for second years is higher in the next two most study time groups.\nIts hard to argue that the third years are spending more time studying."
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#sample-size-appropriate-plots",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#sample-size-appropriate-plots",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Sample size appropriate plots",
    "text": "Sample size appropriate plots\nWhen a plot performs a statistical transformation of a variable, be aware of the sample size used to calculate the transformation. In a boxplot 5 numbers are computed to summarise a distribution, if the sample size is small, the boxplot can be misleading:\n\n\n\n\n\n\n\n\n\nInterpretation: People spending few study hours are spending too much time on the internet.\n\n\n\n\n\n\n\n\n\nCorrection: There’s only three students in the category, of spending too much time on the web and too little time studying."
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#order-of-conditioning",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#order-of-conditioning",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Order of conditioning",
    "text": "Order of conditioning\nWe should condition the response variable (or the variable we are trying to understand) by other variables we think will explain the response. This allows us to examine how the response varies by levels of the explanatory variable.\nGoal: how does core or elective vary by year in school.\nWhich plot is appropriate here?\nThis:\n\n\n\n\n\n\n\n\n\nor this:\n\n\n\n\n\n\n\n\n\nor this:\n\n\n\n\n\n\n\n\n\nExplanatory variable is year in school, and response is core/elective. This tells us that either of the latter two plots is better than the first.\nBecause the counts are different in the years in school variable, the last plot (mosaic) is better. It let’s us know that there are few students in years 1 and 4.\nWhat do we learn? There are a few more third year students doing this as an elective."
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#summary",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#summary",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Summary",
    "text": "Summary\nMaking effective plots can tell you a LOT about data. Its hard! Its an under-rated but very powerful skill to develop."
  },
  {
    "objectID": "posts/2018-04-14-rookie-mistakes/index.html#sources",
    "href": "posts/2018-04-14-rookie-mistakes/index.html#sources",
    "title": "Rookie mistakes and how to fix them when making plots of data",
    "section": "Sources",
    "text": "Sources\nData and code"
  },
  {
    "objectID": "posts/2015-12-26-rogers/index.html",
    "href": "posts/2015-12-26-rogers/index.html",
    "title": "Better cricket plots",
    "section": "",
    "text": "I’m sitting watching cricket tonight, the first day of the Australia vs West Indies Boxing Day test. Just now video of retired batsman Chris Rogers being honored was played, along with a plot of his batting record, shown on screen similar to this one below:"
  },
  {
    "objectID": "posts/2015-12-26-rogers/index.html#howzat",
    "href": "posts/2015-12-26-rogers/index.html#howzat",
    "title": "Better cricket plots",
    "section": "Howzat?",
    "text": "Howzat?\nWhat are they trying to show? What’s the data in this plot? Is it a bar chart? A histogram? What does color mean?\nSequentially, the scores for Rogers test match innings are shown. The height of the bars represent the runs scored in each innings. The horizontal axis is a temporal. It is not a true representation of date that the innings was played, but a simple integer 1 through 48 ordering the innings that he played in time. Color redundantly represents score, e.g. blue means more than 100 runs.\nBarcharts like this are being used to display various statistics during a match, across matches, and types of matches, in the commentating. It is not a good display, actually it is the wrong way to show this data."
  },
  {
    "objectID": "posts/2015-12-26-rogers/index.html#re-draw-this-data",
    "href": "posts/2015-12-26-rogers/index.html#re-draw-this-data",
    "title": "Better cricket plots",
    "section": "Re-draw this data",
    "text": "Re-draw this data\nWe pulled a copy of Rogers’ batting records from the web. Using R, we read the data, wrangle it a little to get it into tidier form, and plot the runs by date.\nrogers2 &lt;- read_delim(\"rogers2.dat\", \"\\t\")\nrogers2$Date[seq(2,50,2)] &lt;- rogers2$Date[seq(1,49,2)]\nrogers2$Date &lt;- as.Date(rogers2$Date, format=\"%d/%m/%Y\")\nrogers2$Vs[seq(2,50,2)] &lt;- rogers2$Vs[seq(1,49,2)]\nrogers2$Ground[seq(2,50,2)] &lt;- rogers2$Ground[seq(1,49,2)]\nrogers2$Test[seq(2,50,2)] &lt;- rogers2$Test[seq(1,49,2)]\nqplot(Date, Runs, data=rogers2, size=I(3), alpha=I(0.5)) + theme_bw()\n\n\nMessage 1\nRogers’ first two innings were in 2008. There was a long spell where he did not play test cricket - probably dropped because he performed badly in his first opportunity. The long period in Sheffield Shield cricket may have helped him mature into a better batsman when he got his second opportunity. It would be better to remove the 2008 data, so that we can focus on his performance once he was consistently on the field.\n\n\nMessage 2\nWhy do the cricket statisticians use bar charts for the runs plotted against the innings index? What are the alternatives? One main reason is that the bar has more ink on the page than a dot and so gives more weight to the number. Its not completely incorrect to use a bar for runs in cricket, after all they do start batting at 0 and accumulate runs in integer amounts. But it incorrectly puts too much emphasis on the large totals scored by the batsman because the low scores get less weight on the page. This is misleading. To evaluate a batsman’s performance we need to see roughly how much they score on average, and how varied are there scores, i.e. they reliably score well.\nWhat is really wrong with these plots is that they use an index for the horizontal axis to place the bars. Its a proxy for time. But as we have seen with Rogers, if there is a big gap in between innings played (2008-2013) we don’t see this in the plot. Why not plot the date the innings was played? Lets re-make the bar charts with date on the horizontal axis and see what happens.\n\n\nMessage 3\nqplot(Date, Runs, data=rogers2, geom=\"bar\", stat=\"identity\") +\n  geom_hline(yintercept=100, colour=\"grey70\") +\n  geom_hline(yintercept=50, colour=\"grey70\") + \n  theme_bw()\n\nWe can now see the big gap in Rogers test career. Ok, we know this now, so lets trim the data, and remove these two innings and focus on his performance on his return to the national team.\nrogers2 &lt;- rogers2 %&gt;% filter(Date &gt; as.Date(\"01/01/2010\", format=\"%d/%m/%Y\")) \nqplot(Date, Runs, data=rogers2, geom=\"bar\", stat=\"identity\") +\n  geom_hline(yintercept=100, colour=\"grey70\") +\n  geom_hline(yintercept=50, colour=\"grey70\") + \n  theme_bw()\n\nSomething looks wrong: Rogers only scored 5 test centuries, but there are more peaks than this. Maybe some innings were completed on the same date, and the runs for these innings were aggregated. We can check this using dodged bars.\nqplot(Date, Runs, data=rogers2, geom=\"bar\", stat=\"identity\",\n      position=\"dodge\") + \n  geom_hline(yintercept=100, colour=\"grey70\") + \n  geom_hline(yintercept=50, colour=\"grey70\") + \n  theme_bw()\n\nThe 50 and 100 mark guide lines are drawn over the bars. To fix this we need to use ggplot instead of qplot.\nggplot(data=rogers2) + \n  geom_hline(yintercept=100, colour=\"grey70\") + \n  geom_hline(yintercept=50, colour=\"grey70\") + \n  geom_bar(aes(x=Date, y=Runs), stat=\"identity\",\n      position=\"dodge\") +\n  theme_bw()\n\nUsing date we can also see the seasonality of the game, Rogers scores runs only around new year and mid-winter. The gaps are a distraction, because no data was generated in these periods, so we would really like to ignore these for the plot. This is one reason why the cricket statisticians resort to an index for the x axis. The other main reason, I suspect is because they are using excel, and this software makes it easy to use an index on an axis.\nThe tool that people use often dictates the type of plots they make. Whatever is easy to do in the tool is likely the product that is created. A better approach is to start with what we want to learn, design the appropriate plot(s) for uncovering this information, and shape the tool to make the appropriate plot(s)."
  },
  {
    "objectID": "posts/2015-12-26-rogers/index.html#a-complete-overhaul",
    "href": "posts/2015-12-26-rogers/index.html#a-complete-overhaul",
    "title": "Better cricket plots",
    "section": "A complete overhaul",
    "text": "A complete overhaul\nThings that we’d like to know about the batman’s innings include:\n\nperformance by seasons\nruns scored consistently and reliably scoring runs\nwhether performance depended on the opponent or location\nfor long term players is performance up-trending or down-trending\nthe proportion of the team’s total that this score represents\n\n\nAggregating\nThere is not a lot of data in a single match since a batsman only bats at most two innings, and two points is not enough data to calculate statistics. Aggregating in different ways on time can help pull enough data together to better assess performance. The natural ways to aggregate temporal data are day, week, month, quarter, year, none of which work for the scale of cricket.\nSometimes we hear a commentator say a player is having a good season. Maybe we can think about test cricket as seasons, summer Australian time, England time, and the occasional shoulder season in the spring or autumn. We can create a variable to reflect these and show runs by season.\ncricket_season &lt;- function(x, with_year = TRUE){\n  m &lt;- month(x)\n  seasons &lt;- c(`1` = \"a.summer\", `2` = \"a.summer\", `3` = \"b.autumn\",\n               `4` = \"b.autumn\", `5` = \"b.autumn\", `6` = \"c.winter\",\n               `7` = \"c.winter\", `8` = \"c.winter\", `9` = \"d.spring\",\n               `10` = \"d.spring\", `11` = \"a.summer\", `12` = \"a.summer\")\n  if (isTRUE(with_year)) {\n    q &lt;- unname(seasons[m])\n    y &lt;- year(x+months(2))\n    q &lt;- paste0(y, \".\", q)\n    q &lt;- factor(q)\n  }\n  else \n    q &lt;- unname(seasons[m])\n  return(q)\n}\nrogers2$Season &lt;- cricket_season(rogers2$Date)\nqplot(Season, Runs, data=rogers2, size=I(3), alpha=I(0.5), \n      position = position_jitter(width = .1)) + \n  geom_hline(yintercept=100, colour=\"grey70\") + \n  geom_hline(yintercept=50, colour=\"grey70\") + \n  geom_hline(yintercept=5, colour=\"grey70\") + theme_bw()\n\nLooking at the performance by season, Rogers best season was summer 2014 when he scored three centuries. We can also see that in this season in five innings he also scored less than 5 runs. His scoring is a mixed bag in every season, some very disappointing innings balanced with some very impressive run scoring.\n\n\nBy opponent\nqplot(Vs, Runs, data=rogers2, geom=c(\"boxplot\",\"point\")) + \n   theme_bw() \n\nMost of Rogers’ innings were played against England. Against India he scored more consistently. Against other opponents he was as likely to go out for a low score as a high one.\n\n\nIs first or second innings better?\nrogers2$id &lt;- rep(1:24, rep(2,24))\nqplot(Inns, Runs, data=rogers2, group=id, geom=\"line\") + \n   facet_wrap(~Vs) + theme_bw() \n\nHere we plotted score against innings number. Lines connect the two innings for a match. The main pattern that we see for Rogers is a lot of crossed lines. This means that if he scored well in one innings he tended to score not so well in the other innings. When a line goes up, the score in the first innings was lower than that in the second, and a downward sloping line indicates better first innings score than second. If he consistently scored the lines would be relatively flat. Against India, Pakistan and South Africa we could say this was true, that his innings scores were consistent. Against England, it was one up, one down.\n\n\nRun rate\nTest cricket is not a time-restricted game and batsmen typically have the luxury of building up their runs as the conditions allow. But it can be interesting to examine runs scored by the number of balls faced, which is the strike rate. The higher the strike rate the faster the runs are coming. Below Roger’s strike rate (\\(x100\\)) is plotted by season. A value of 50 means that one run is score for every 2 balls faced. A value of 100 means one run per bowl.\nrogers2$`B/F` &lt;- as.numeric(rogers2$`B/F`)\nrogers2$`S/R` &lt;- as.numeric(rogers2$`S/R`)\nqplot(Season, `S/R`, data=rogers2) + \n  geom_hline(yintercept=50, colour=\"grey70\") + theme_bw() \n\nRogers run rate hovers around 50. The summer where he scored the three centuries his run rate was mostly lower than 50. His run rate was highest in the summer of 2015.\nLooking at run rate by the innings total could be interesting also. This is shown below.\nqplot(Runs, `S/R`, data=rogers2) + \n   geom_smooth(method=\"lm\", se=F) + theme_bw() + theme(aspect.ratio=1)\nqplot(Runs, `S/R`, data=rogers2) + \n   geom_smooth(method=\"lm\", se=F) + \n  facet_wrap(~Vs) +\n  theme_bw() + theme(aspect.ratio=1)\n \nRogers’ strike rate increases with larger innings scores. It looks like at some point during these innings his confidence builds and he goes for more shots to yield overall faster scoring. His rate shows a similar pattern for all opponents. He has one unusually high strike rate of about 125. This is an anomaly compared to his other performances and may have been a time when he was told to go in have a quick bash to rack up some runs, before a declaration."
  },
  {
    "objectID": "posts/2015-12-26-rogers/index.html#summary",
    "href": "posts/2015-12-26-rogers/index.html#summary",
    "title": "Better cricket plots",
    "section": "Summary",
    "text": "Summary\nI recommend showing dotplots of the runs by season, instead of bar charts by index. Ideally overlaying summary statistics such as averages and standard deviations to simplify comparison across seasons, or in the plot below the interquartile range, which shows the middle 50% of scores.\nseasonstats &lt;- rogers2 %&gt;% group_by(Season) %&gt;% summarise(m=median(Runs, na.rm=T), q1=quantile(Runs, 0.25, na.rm=T), q3=quantile(Runs, 0.75, na.rm=T))\nggplot() + \n  geom_hline(yintercept=100, colour=\"grey90\") + \n  geom_hline(yintercept=50, colour=\"grey90\") + \n  geom_hline(yintercept=0, colour=\"grey90\") + \n  geom_errorbar(data=seasonstats, aes(x=Season, ymin=q1, ymax=q3), \n                width=0, alpha=0.7, color=\"black\", size=12) +\n  geom_point(data=rogers2, aes(x=Season, y=Runs), size=5, alpha=0.6,\n             colour=\"red\",\n      position = position_jitter(width = .1)) +\n  theme_bw()\n\nIf you really insist on using index on the horizontal axis, these would would be my suggested enhancements: use points instead of bars, a graduated or sequential color scale to highlight the high scores, and add a mean and/or a median career score as a reference line.\nrogers2$Test[seq(2,50,2)] &lt;- rogers2$Test[seq(1,49,2)]\nrogers2$Test &lt;- as.numeric(rogers2$Test)\nrogers2$milestone &lt;- \"&lt;50\"\nrogers2$milestone[rogers2$Runs&gt;49] &lt;- \"50s\"\nrogers2$milestone[rogers2$Runs&gt;99] &lt;- \"100s\"\nrogers2$milestone &lt;- factor(rogers2$milestone, levels=c(\"&lt;50\", \"50s\", \"100s\"))\nmn &lt;- mean(rogers2$Runs, na.rm=T)\nmd &lt;- median(rogers2$Runs, na.rm=T)\nqplot(Test, Runs, data=rogers2, size=I(4), alpha=I(0.9), color=milestone) +\n  scale_color_manual(\"\", values=c(\"&lt;50\"=\"grey60\", \"50s\"=\"orange\", \"100s\"=\"red\")) +\n  geom_hline(yintercept=md, color=\"grey80\", \n             alpha=0.5, size=3) +\n  annotate(\"text\", x=22, y=md-5, label=paste(\"Median runs=\",md), color=\"grey80\") +\n  theme_bw()\n\nIn terms of available data, if the data table on the web contained the team total for innings it would be easy to see how much the batsman contributed to the team’s effort. If the country of the cricket ground was included in the data table we could examine a location effect on performance. These could be pulled from other sources, but I’ll wait until another blog post to discuss this.\nAlso in future blog posts this Australian summer, look for some other plot recommendations for types of matches, one day internationals, T20, big bashes, and for bowlers, comparing batsmen and teams. I’d like to also explore the women’s cricket data but this is currently very sparse.\n\nAcknowledgements\nThe data was extracted from http://www.howstat.com. Plots were made using these R packages: dplyr, ggplot2, knitr, lubridate, readr, tidyr."
  },
  {
    "objectID": "posts/2024-07-14-R-devel/index.html",
    "href": "posts/2024-07-14-R-devel/index.html",
    "title": "Tinkering with the core R code is easier than it used to be, thanks to R Dev Container",
    "section": "",
    "text": "R Developers Day, organised by Heather Turner and Shannon Pileggi followed a very successful useR! 2024 in Salzburg, Austria. This was the opportunity to work alongside experienced programmers on contributions to base R.\n\nThe setting of Paris Lodron University of Salzburg (PLUS) is gorgeous, on the Sound of Music side of Salzburg, with views to the fortress across a field of wheat.\nThis post summarises what I learned about how to make changes in base R, compile and check the results."
  },
  {
    "objectID": "posts/2024-07-14-R-devel/index.html#context",
    "href": "posts/2024-07-14-R-devel/index.html#context",
    "title": "Tinkering with the core R code is easier than it used to be, thanks to R Dev Container",
    "section": "",
    "text": "R Developers Day, organised by Heather Turner and Shannon Pileggi followed a very successful useR! 2024 in Salzburg, Austria. This was the opportunity to work alongside experienced programmers on contributions to base R.\n\nThe setting of Paris Lodron University of Salzburg (PLUS) is gorgeous, on the Sound of Music side of Salzburg, with views to the fortress across a field of wheat.\nThis post summarises what I learned about how to make changes in base R, compile and check the results."
  },
  {
    "objectID": "posts/2024-07-14-R-devel/index.html#getting-started",
    "href": "posts/2024-07-14-R-devel/index.html#getting-started",
    "title": "Tinkering with the core R code is easier than it used to be, thanks to R Dev Container",
    "section": "Getting started",
    "text": "Getting started\nI recommend that you have a block of time (1-2 hours) to get up and running, because once you start you’ll want to keep tinkering with the base code for a while. And if you leave the work for a half hour or so, the session will time out and may error when you try to start it up again, at least this is what has been happening to me as I try to reconstruct the day’s efforts.\nTo get started with a fresh codespace:\n\nPoint your browser to https://github.com/r-devel/r-dev-env\nClick on “Open in GitHub Codespaces”\n\n\n\nthen “Create codespace”. This can take a few minutes to complete.\nNow you want to get a local copy of the R source, into your container, and build it. The instructions on doing this are in the docs, tutorials, building_r.rmd file.\n\n\n\nFollow the instructions, from step 2 onwards, by cutting lines of code from the instructions and pasting into your TERMINAL window. This means doing this:\n\nsvn checkout of the source\ndownload the recommended packages\nchanging the build directory\nconfiguring the build\nthen making\n\nRun the devel version of R! You can use the which_R command and choose r-devel to set it.\n\n\n\nTo run R so that you get a graphics window, click the R (not attached) on the bottom right of the window.\n\n\n\nThe base code is in the svn/src directory"
  },
  {
    "objectID": "posts/2024-07-14-R-devel/index.html#what-i-did-enhance-base-graphics",
    "href": "posts/2024-07-14-R-devel/index.html#what-i-did-enhance-base-graphics",
    "title": "Tinkering with the core R code is easier than it used to be, thanks to R Dev Container",
    "section": "What I did: “Enhance” base graphics",
    "text": "What I did: “Enhance” base graphics\nBase graphics code is in the library/graphics directory.\n\nChanging R code to create better histogram defaults\nA good place to start is modifying base code that is directly written in R, such as the histogram function. It is in the file R/hist.R. You can open and edit this file.\n\n\nMake a change, like the default boundary to be white:\nThen make in the terminal window to build R again\nRestart R and check your work with\n\nhist(runif(104))\n\n\nBefore change\n\n\nAfter change\n\n\n\n\n\nChanging C code to remove the one case dependency\nThe next step is to tackled changes in C code. Interestingly, although the histogram code is directly written in R, the stem and leaf code is in the C code base.\n\nEditing this code is fairly straight forward, though, and doesn’t require much C knowledge.\nThis project arose from Ella Kaye’s presentation at useR! 2024 pointing to deficiencies in the current stem-and-leaf display. For example, if your data has only one observation there will be no result. The code has a line checking for the number of observations, and simply returns if n &lt;= 1 (see the image above).\nCommenting these lines out with // results in the stem-and-leaf displayed for a single observation.\n\n\nBefore change\n\n\nAfter change\n\n\n\nIt is tempting to make many changes to the stem-and-leaf code for example, if the data is reasonably sized and the number of characters available on the line is less than the number of leaves in a stem, it will simply stop printing more characters and finish with +21 where the number indicates how many more characters should have been printed.\nAnyway, who uses stem-and-leaf any more. Tukey had a gazillion varieties of them, but they are more useful when working with pencil and paper."
  },
  {
    "objectID": "posts/2024-07-14-R-devel/index.html#where-to-go-from-here",
    "href": "posts/2024-07-14-R-devel/index.html#where-to-go-from-here",
    "title": "Tinkering with the core R code is easier than it used to be, thanks to R Dev Container",
    "section": "Where to go from here",
    "text": "Where to go from here\nMaking changes to base R is much less intimidating that I realised, particularly with the R Dev Container developed by Atharva Shirdhankar with mentorship by Heather Turner, James Tripp, Iain Emsley.\nThis opens the possibility of reviewing, analysing and fixing bugs in code or documentation reported on R’s Bugzilla. This is a good opportunity to contribute back to the community, and for exercises or assignments in advanced programming classes. Any patch reported will get substantial scrutiny from an R Core member before it ias accepted. It was exciting to see several patches become part of the R base over the course of R Dev Day. I did not submit any patches of my base R graphics enhancements 🤪.\nWarning: I quickly burned through my 60 hours per month of GitHub CodeSpaces. I don’t think that I was close to this amount of use, but maybe it happened by opening new codespaces trying to find the one I started with. This site will show the codespaces you have created, and allow creating new ones, and possibly whether you have remaining time on your monthly plan to keep playing."
  },
  {
    "objectID": "posts/2018-10-10-useR2018/index.html",
    "href": "posts/2018-10-10-useR2018/index.html",
    "title": "The background to useR! 2018",
    "section": "",
    "text": "useR! 2018 was held for the first time in the southern hemisphere, and the feedback from participants has been very positive. I have been asked to write about the organisation and this is a good way to get some of the planning and decisions and operations into print, so that it might be useful for others charged with conference organisation. There are a lot of people who made the conference a success, and their contributions need to be acknowledged. A philosophy underlying the planning was to have some fun, use open source tools, and experiment a little, creatively.\n\nLocation\nA twitter poll was conducted prior to submitting a proposal to host useR! 2018. Choices were Cairns, Brisbane, Sydney or Perth. Brisbane was the overall favorite, a balance between reasonable weather, and reasonable travel.\nNick Tierney (with some help from Jessie Roberts and Miles McBain) scoped out the Brisbane Convention and Exhibition Centre (BCEC). Their marketing team jumped on board immediately, providing us with a glossy brochure with information on facilities, travel, accommodation, tourism, prices and even negotiated a grant from Brisbane City to help our finances.\nSince the last two conferences, at Palo Alto and Brussels, had closer to 1000 attendees, a main concern about the location was size. The Brisbane convention centre auditorium can hold 620 people. We estimated that this might be ok. We gambled that it would be unlikely to manage to get these numbers, all the way to Australia. but we had a backup option of doing a live feed of keynotes into a second conference room. Fortunately, everyone fit, and filled the auditorium.\n\n\nGeneral organisation\nWe created a slack group for communication about planning for the conference. By the end of the meeting there were 18 channels, basically sub-committees for different purposes. Initially I asked a few people to help, particularly with the development of the proposal. Once, it was announced that it would be held in Australia, I had a lot of people who volunteered to help with the conference organisation. This was absolutely spectacular! It is a tribute to the R community that people are so generous with their time. For the most part if someone contacted me wanting to help, I included them, until it came to a time when the number was getting a bit unwieldy. We needed to make sure that as tasks needed to be done, committee members would actually realise that despite the large number of helpers, that someone needed to step into and take the lead on crucial jobs. Maintaining a diverse organising committee was also of primary importance. A diverse committee helps to ensure that the conference will be a welcoming environment to all R users.\nMost communication about the organisation was done through slack. I really like it. There is a bit of a problem sometimes that messages don’t necessarily get to people unless the person is tagged, and it generally requires that members check in to see communications. If a person is tagged, and they don’t respond slack will send an email. I like it because it helps with transparency, there is a log of communications in one place, and because members can choose when to check in, it’s not intrusive.\n\n\nLogo development\nKangaroo and sun are iconic Australian images, and right from the start it was clear that these needed to form the core. We wanted to use a female kangaroo with a joey popping up form the pouch, because the baby in the pouch is the really cute aspect of many Australian animals. I experimented using an image available on the web, but it was not creative commons. The only creative commons images were of male kangaroos. So I searched my own photo collection, without success. Lots of kangaroos with joeys but the joey’s head was flush with the body which doesn’t work in silhouette. My sister has a much larger photo collection from all her plant and insect collecting trips to the wild remote parts of the country. She is also good at drawing, so she spent several hours making initial sketches, based on merging several photos. The final version was one of these that I modified some more. Göknur Giner cleaned up the edges and colours using Adobe Illustrator one weekend.\nThis was supposed to be a base image, and then we would add location and date using an R code like was done by the Aalborg team for useR! 2015. The image would just be the icon. In all the hectic months leading up to the conference, particularly in semester 2 2017 (Aug-Nov), when some of the critical planning work needed to be done, I was teaching two classes, one for the second time and the other a new class, totalling 300 students. I only remembered again after the conference finished that I’d forgotten.\n\n\nWebsite\nA goal was to do as much as possible of the conference tools and facilities entirely with R. We made a github account. The web site was made using the blogdown package, with materials kept on github, and deployed using netlify. These are fairly new tools, that many people are adopting because it is easy to create elegant web sites, and maintain a blog. Earo Wang led the team on the web site, creating the initial design, writing new css, and javascript for some special needs, making the hex sticker background image. She also automated the pulling abstract information out of google sheets, and converting directly into html format for display on the website. Her infrastructure made it easy for me to make many little modifications.\n\n\nKeynotes\nWe solicited input on keynotes using an online survey just after the previous useR! 2017, also requesting suggestions for the conference (the primary one was to be sure to have a conference t-shirt!). There were many more men’s names provided to be keynotes, so Rob Hyndman and I sat together and brainstormed a more diverse set of names. In mind was a comment on the foRwards chat by Alicia Oshlack from several years ago that there was no excuse not to have a 50:50 gender ratio. That was the goal, and we chose to have 6 keynotes to achieve this. There are women in the R community who have done and are making exceptional contributions. It’s really not hard to find them. Along with gender, it was important to balance geographic location (local, USA, Europe) and age, a mix of relatively junior and senior researchers. Another criteria was evidence of ability to give engaging and thought-provoking presentations, so we checked for online videos of the potential speakers, and refined the list based on the quality of these recorded talks.\nRob Hyndman took the lead on inviting speakers.\nA hiccup happened two weeks before useR! 2018, when Heike Hofmann had to pull out because of visa issues. We were especially lucky to be able to recruit Dani Navarro to step in, thanks to Steph De Silva. Dani made a brilliant contribution to the programme.\n\n\nSponsorship\nWe were very late working on getting sponsors. I was really lucky that once Steph de Silva had agreed to be a keynote speaker, she asked, “So what can I do to help?” Our sponsorship actions were really lagging. Alex Whan and I were working on this but I think we were both a bit green. Working on getting sponsors was one of the areas where I told Steph we needed help. Her first response, was “So, have you got a prospectus?” To which I promptly responded, “What’s that?” Steph and Alex whipped up the most professional-looking brochure with details on levels of sponsorship in terms of funds and opportunities. Alex set up a trello board based on the list of sponsors and contacts from last year’s conference.\nOur very first sponsor was Mark Dulhunty from Educational Measurement Solutions, with an inquiry offering to sponsor child care! Yes! Goknur Giner, soon to become a Mum, took the lead on organising child care for the meeting. We had already received information from the BCEC about a nanny service who regularly worked with them. Goknur followed up on this, and got us all sorted with quotes, before her baby came.\nTechnically, the Monash Business School was our first sponsor. After it was announced that we would be hosting, it dawned on me that there were some serious expenses that we would need to cover pretty early, especially to lock in the facility. I was very lucky that my first conversation with our department manager, and then the events team, provided a huge safety net. They immediately offered $25k of sponsorship (the first platinum sponsor) and assured me that we could cover the early expenses, and overspend as needed.\nAlex and Steph worked their way through the trello board of contacts sending emails inviting sponsorship. We linked the brochure to the web site and made a blog post and tweeted. RStudio were the first big sponsor to come to the table, just after the RStudio conference in January. At the time Hadley Wickham ribbed me, “So who made the brochure? It clearly wasn’t you!” Yep, definitely not me, it’s brilliant, isn’t it!\nSteph made many follow up skype calls, talking with R Consortium’s lead, John Mertic and Microsoft’s Terry Christiani, and many others over her intermittent bush internet. I remember the conversation with Terry being interrupted at least once. But another thing from that conversation was a warning from Terry that Microsoft is notoriously slow with getting funds transferred. True to this, we have only just received the $25k that was promised, lots of bureaucratic hiccups.\nSteph recognises, and this comes across in her conversations with potential sponsors, the benefit that sponsors get from supporting an event like useR! 2018. Being able to convey this, and work with sponsors to get a solution that suits their needs too, like the bag deal with VergeLabs, the extra diversity scholarships from NAB.\nIt was our events temporary staff member, Jess Gully, who managed the tough work of getting contracts processed.\nThe sponsors made a big difference for this year’s conference particularly being financially viable, and we hope that they all feel they received the visibility and appreciation they hoped for.\n\n\nTutorials\nThe input survey also asked for suggestions for tutorial topics and potential presenters. This formed a basis for the invited tutorials. We then invited applications. A google form was designed for this, with the goal being to directly download the information into R to do the reviewing.\nEaro Wang and Mitch O’Hara-Wild had a brainstorming session, about how nice it would be to have a shiny app to handle tutorial, scholarship and abstract reviewing. Mitch took up the challenge and produced cooee. The github site has three branches with variations on the app for the different purposes. Mitch has promised to write a blog post, or an R Journal article, about this app, and possibly make it available on CRAN.\nWe had 66 tutorial applications. This may have been because for the first time we offered some travel support for presenters, up to $3000 for international and $1500 for domestic. We asked for some demographic information from applicants. Again there were many more submitted by men. We wanted to have balance of gender, a broad selection of topics to cater to the diverse interests of the R community, quality material, with a hands-on focus so participants actively learned. There were 6 rooms guaranteed to be available (including the auditorium which was not ideal for a hands-on workshop) which allowed 18 tutorials. About 4-5 slots had already been taken for invited presenters – I don’t remember exactly the numbers because invited and accepted presenters were treated equally in the organisation and reimbursement – which meant that a lot of terrific proposals had to be turned down.\nJust prior to the meeting we organised helpers for each of the tutorials. This is a good opportunity to learn on the go, particularly students. We had many on the programme committee that stepped into these roles, and where we needed more support, I leaned on our wonderful Monash PhD students to help.\n\n\nScholarships\nA google form was constructed to collect information for awarding scholarships, details were hashed out between the scholarships team. We had budgeted $10k from the BCEC funding, and then funds from several sponsors supplemented this. The decision was made to spread the funds more widely than deeply, hoping that recipients might be able to find additional funds to help cover the rest. This amounted to $1000 for international and $500 for domestic travel, plus free registration. We had almost 100 applicants! There is clearly a need for funding travel scholarships.\nWe asked applicants:\n\nto tell us about themselves, particularly their R and open source or open data activities\nwhether they identified as an under-represented minority, socially or geographically\nwhy they are applying for a scholarship\nwhy attending useR! 2018 would be helpful for them, and impact the community\nwhere they would be coming from\n\nThe scholarship review team read all of the applications and rated them on the basis of region they would be representing, record of activity in the R community and potential impact on their local community after the meeting. A more complete summary of the procedures can be found here.\nIn the end we awarded 7 international and 9 domestic scholarships. There was one hiccup, and my recommendation for the future is to keep in touch with awardees to make sure that they are going to come, and for awardees to communicate with the organisers. Two of the successful applicants from Africa registered for useR! 2018 with the free code right after receiving the scholarship, but I learned just before the conference that they had decided that they could not afford it. If we had known much earlier we could have awarded the funds to another applicant, or possibly found additional funds.\n\n\nAbstracts\nAbstracts were submitted through a google form, which allowed our programme committee to the review them through our shiny app cooee. There were more than 200 abstracts submitted. The review team (Anna, Paula, Soroor, Miles, Nick, Thomas, Alex, as well as me) made a massive effort to work through the abstracts. For talks, we had at least two people provide a review, and for posters one was sufficient. A really big thank you to all of the organising committee members who worked really hard in the few weeks before the early registration deadline to get the abstracts reviewed.\nCooee, the shiny app for reviewing abstracts, had several key features:\n\nOnly the title, abstract and keywords were visible to reviewers. An administrator could look at names and demographic details after the reviews were done. Mitch and Earo were adamant that reviews needed to be conducted blind, but the final decisions could take this information into account.\nAbstracts were rated into one of four groups, using Australianisms “bloody ripper”, “beaut”, “okey-dokey”, “sorry”, the equivalent of outstanding, good, ok, not good enough. This made what is a bit of a chore, fun, especially to be able to label an abstract as a “bloody ripper”. (The origin of this term is explained here.) The groups were also colour coded in the app, which helped quickly see the reviewer feedback (green for best, red for reject).\n\nThe submission rates by gender were not equal among the three types of submissions: talk, lightning talk, poster. There was a higher fraction of women who submitted posters than the other categories. We encouraged a few of these women, with topics that would lend themselves well to presentations, to switch making a presentation, once their abstracts had been accepted.\nAbstracts were kept separate from registration because we felt that people needed to know if their presentation was accepted before registering and making travel plans. We planned to use the email addresses to link the records, and didn’t anticipate the number of people who would use different emails for the two purposes! It was a major nuisance reconciling the two spreadsheets, with the primary difficulty ensuring that presenters on the programme had registered for the conference. We merged the sheets using tidyverse tools, to identify mismatches, and sent emails to these people. Hannah (our Monash events staff member) worked laboriously on this manually in the weeks before the conference. In retrospect, the abstract form, or acceptance email, would ideally have provided a code that needed to be entered when registering, to help keep the two lists coordinated. One of the complications in registration and abstracts was that some government departments do not allow their staff to register for an event in different financial years - so some presenters could not register until after Jul 1st!\nOrganising the abstracts into sessions is a real challenge! The first approach was to use topic models using tidytext to read and process the title, keywords and abstracts and the package topicmodels to create topics. We learned a bit about topic modeling: you really need a lot more text, and a lot more documents. The approximately 125 talk abstracts with a huge variance in content doesn’t reduce in dimension very well. The final approach was to use the keywords, and the package upsetR to organise the abstract keywords into sets. We primarily needed blocks of four, because all but two sessions had space for four talks, and then 10 sets of three for the two shorter sessions.\n\nFrom the plot here, you can see that “community/education” was the only keyword used for 8 accepted abstracts, so these formed two natural sessions. “Applications” was used solo in two abstracts, then there were two more abstracts that used “applications” and “algorithms” so these four were grouped together. The motto here is “pay attention to the keywords that you use to tag your work”! In my years as an editor I have seen a huge variability in the attention paid to keywords in papers, some with none, some with a set that have no relationship to the paper, some with 20. A good set of keywords can help place your work in the field quickly.\nNick and I did the lion’s share of the grouping of abstracts, and we pulled in help to make some of the tougher decisions. Earo built the infrastructure to get the abstracts into google docs and then onto the web site and also into the mobile app. Organising the timing of sessions was the next challenge and making up reasonable names to roughly match the keywords of the abstracts was fun. We needed to make sure that a parallel session had as different a range of topics as possible, that two sessions on, for example, “community/education” were not scheduled at the same time.\nYou would think that there might be some good tools for doing this sort of work available. We found some commercial systems, but it seems no open source. A good opportunity for someone to make a contribution here! It was a good exercise in text analysis from my perspective: I learned a lot, and I think that Nick and Earo did also.\n\n\nWelcome to country\nAustralian history is fraught with shameful relationships with the original inhabitants of the country. A Welcome to Country ceremony derives from the traditional way that newcomers to a territory learned of the food, water and resources available in the area and acknowledged the current owners. Jess sought out the services of the wonderful Songwoman Maroochy to conduct this ceremony to kick off useR! 2018.\n\n\nSwag\nNick Tierney spearheaded the swag. Early on we decided that we wanted conference socks - many of us have a passion for cute socks. Also, it is winter in Australia in July, and although Brisbane has lovely winter weather, it can be a bit chilly in the mornings. A pair of socks might be appreciated. Another goal was to have Australian-made products, if possible. I saw LaFitte socks in a shop when browsing around a country town, and took a picture for Nick. From the start the company was terrific to work with. We had an initial plan to use the logo in a hexagon-tiled design, but this was too complicated. Two colours only were easier to work with. I was pretty keen to design the socks ourselves (along with the T-shirt and bag design), which was a bit shocking for Nick. It could have been disastrous, but wasn’t. The hex tiling was easy. We wanted bright colours: orange an blue were themes in the logo. Alternating between a kangaroo and an R in the hexagons seemed reasonable, and we mocked up a design using Keynote. Nick went back and forth with LaFitte a couple of times about the hexes needed to be a minimal size, and the kangaroo with the joey was too detailed for their weaving pattern. Mitch found an free use public image of a silhouette that was suitable with a few tweaks. In the first mock up by LaFitte the kangaroo looked like a hammerhead shark! Shifting which pixels were coloured blue a tad had it looking more kangaroo-like. The colours available were more limited than we expected, and finding the closest match to our logo colours was tricky. The end result was a bit darker than expected.\nThe t-shirt and bag design needed to have a limited number of colours for ease of printing and cost, so the logo couldn’t be used. The design was inspired by Hadley Wickham’s design work for useR! 2007, where he pulled the names of all R packages and arrange them into an R shape. There are a lot more packages today, which makes it impossible to use the entire list. Mitch pulled the names of top downloaded packages from the RStudio CRAN mirror, using the cranlogs package, loaded these into www.wordclouds.com with our kangaroo and joey (slightly modified) as the layout shape. The output was saved as an image and loaded into Adobe Illustrator to add additional text.\n\n\nPosters\nMost poster abstract submissions were accepted. There is generally more freedom to put a poster up because it primarily just needs space. We planned for a max capacity of 40 posters. In the end was a bit less. I think its more common to have dropouts in this area of the programme, because not presenting a poster doesn’t really impact anyone. Anyhow, we had 40 accepted, and then realised, that this meant we needed to make the space. This is too many to host in one session, so it did need to be broken into two. And we needed facilities for holding the 20 posters.\nWe had to contract out from the BCEC to get the poster materials (and exhibitor booths) because they don’t provide it. The first quote was for close to $20,000 to get TVs to do electronic posters, and about $3000 to get poster board stands for paper posters. The cost for TVs was a bit shocking - that’s $1000 per TV - I can buy a good one for that amount. It made no sense. Two more quotes came in around the same price. I was really keen to do electronic posters, though. I threatened to go out and buy the TVs myself, and have them delivered to BCEC, and set them up myself, and give them away as prizes or to charity after the event. I contacted Radio Rentals, a nationwide company, who provide primarily domestic rentals - we used them for a fridge, washer and dryer when we first moved to Australia. The store in Brisbane offered to provide 20 TVs for 3 days for $3k. We would also need to rent equipment to stand the TVs on, and we would need to do the set up ourselves. The BCEC was very much against using Radio Rentals, though. At least this gave us a lower bound for the cost. And with some additional work Hannah and Jess found a company from Sydney willing to provide the TVs, stands and setup for under $10k.\nWe also made room to have a poster lightning advertisement after the first keynote on the first day. All poster presenters had the option to do a 30 second, one slide, advertisement of their poster. These presenters did a great job, and there were many people gathering around posters during both sessions.\n\n\nCode of conduct\nNick, Rhydwyn and Paula, pulled together examples from http://ozunconf17.ropensci.org/coc/ (that Nick pulled together from the ROpenSci CoC which was inspired by the ADA Initiative) and the CoC from useR! 2016. Other CoC’s examined include Django 2016.\nSix members of the organising committee agreed to be members of the response team. I would recommend that members have some training prior to the conference. The Monash team members, along with events team staff, attended a course on “Responding to disclosures of sexual assault” offered by Monash’s Safer Community Unit. Rhydwyn also suggested the Sage Sharp who have been used to train responders for linux and python conferences.\nThe final CoC was linked prominently from the main page of the web site. The events team purchased a mobile phone to be used as a contact number during the meeting, and the amazing Rhydwyn also provided a mobile phone.\n\n\nSurvey\nJessie Roberts took charge of creating the survey of participants for the conference. This follows from surveys conducted at the previous two meetings. Most of the questions came from these conference surveys, to provide some continuity in monitoring the community. We added some questions specific for our meeting. I used wufoo by SurveyMonkey to conduct the survey, primarily because I still had a professional plan, and it is a simple interface with reasonably formatted data outputted.\n\n\nPricing and budget\nThe original proposal estimated costs based on the BCEC price brochure, and incidentals like travel for keynote speakers. We proposed registration fees on the basis of these costs, in order to break even at around 400 participants. The past two useR!s had on the order of 1000 participants, but we expected numbers to be lower.\nThe actual registration fees were similar to what was proposed. The goal was to encourage people to attend and for the fees not to be a barrier. When we rolled out the registration site we also wanted to help a reasonable number of students to attend the conference dinner, so offered the first 50 student registrations, which were already very low rates, free dinner. These disappeared pretty quickly. We also offered some travel compensation for tutorial presenters, because we wanted to have good quality tutorials from international as well as local presenters. We also wanted to provide lunch onsite so that participants would have more chance to hang around and mingle with each other. These were fairly substantial costs to manage.\nRegistrations really came in two main waves, just before the early registration deadline. A big peak! This was quite a relief because its a bit stressful thinking that maybe few people will come, and the responsibility to host the meeting downunder was misplaced. But the numbers were not enough to cover costs, so the financial stress continued through April and May. I kept some calm by reminding myself of reports from previous conferences that there is often a big rush shortly before the meeting. The second registration wave did happen in June. We ended up closing off the registration, because we had reached 600 and we had to manage numbers given to the caterers. But the moment we closed registrations, we had several emails very nicely asking if we could please let them attend, including one scheduled speaker. We opened registration temporarily for a few hours, and did also take a couple of onsite registrations.\nRegistrations were done using an Australian company trybooking.com. The original plan was to use the Monash shop as the registration engine, but after opening registration with that in January, it was quickly obvious that it couldn’t handle conference signups. Hannah scrambled and re-worked the registration site.\nIn the end, we have covered costs and have some additional funds. The additional funds will be contributed back to the R Foundation to help support R development, and future conferences.\nMonash Business School provided a very supportive safety net to the operations. Its a very different kind of organisation, with a strong emphasis on social responsibility, and I am really thrilled and honoured to be a part of this university.\n\n\nHexwall\nA big hit at the conference was the rainbow hexmap of R package stickers in the shape of Australia. Many photos were taken in front of the banner. This idea evolved from a chance discussion during a research meeting on making a hexgrid of Australia for the Australian Cancer Atlas. Steph Kobakian has been working on that project, and we had also worked on similar problems related to displaying electorates. The problem with cancer maps is that most of the regions are tiny, concentrated around the coast and major cities, with a few gigantic sparse outback regions. Its difficult to get a sense of the spatial distribution of cancer using a choropleth map. Hence Steph is working on an algorithm to layout regions into a hexgrid.\nIt was during one of the discussions on this that Mitch suggested, hey why don’t we do a big hexsticker banner for useR! And Earo suggested why not make it into the shape of Australia, too. The first random assignment of hex stickers failed Mitch’s neatness criteria and he came up woth a way of sampling the colour and ordering by this. Details on making the hexwall are at his blog.\n\n\nHex board of name tags\nThe hexwall work was also related to the name tag design, and the way that attendees would collect their name tags. We wanted the name tags to be a bit unique. We gave some initial ideas to out local printer, Anthony Hall of harkmedia, and he came back with a really good first design. A few small tweaks got us nice big hexagon name tags, with dots for tutorial registrations.\nNick had also experienced a name tag hallway at a different conference, which he had found to be a fun way to collect materials. What was possible for us was to have the name tags made from durable material, with velcro dots on the back that could stick them to a fuzzy board. The name badges were arranged approximately in alphabetical order on the hex board, and finding their name badge was the first task for attendees.\n\n\nDatathon\nThe purpose of a datathon is to show off R skills for analysing data. It also creates a dynamic activity for participants, which with prize money, can help to off-set travel expenses. We had a lot of ideas for data sets but we wanted to get some support from the data provider. We started with trying to work with the Australian Bureau of Statistics, but there are so many privacy constraints on this data that we got discouraged. My sister, Professor Lyn Cook from UQ, suggested the Atlas of Living Australia was a terrific and extensive open database. I had a couple of discussions with the curators of this archive, most extensively with Peggy Newman, IT Advisor for the Atlas, who worked with me on putting the guidenines together, and coming to Brisbane to judge the entries. A big plus was the R package ALA4R posted to CRAN just in time for the competition, that allowed drawing samples directly from R."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching resources",
    "section": "",
    "text": "number\ntitle\nlocation\ndescription\n\n\n\n\nETC3250/5250\nIntroduction to Machine Learning\nMonash University\nCovers the classic methods for dimension reduction, supervised classification and cluster analysis. What is special about this unit is the use ot high-dimensional data visualisation to explain emothds, and to diagnose models.\n\n\nETC5521\nDiving Deeper into Data\nMonash University\nThis is an advanced exploratory data analysiis unit, that is based on the spirit of Tukey, to discover the unexpected in data. It has a large focus on making effective plots of data, and checking for suprious structure with modern methods of inference for graphics."
  },
  {
    "objectID": "teaching.html#university-courses",
    "href": "teaching.html#university-courses",
    "title": "Teaching resources",
    "section": "",
    "text": "number\ntitle\nlocation\ndescription\n\n\n\n\nETC3250/5250\nIntroduction to Machine Learning\nMonash University\nCovers the classic methods for dimension reduction, supervised classification and cluster analysis. What is special about this unit is the use ot high-dimensional data visualisation to explain emothds, and to diagnose models.\n\n\nETC5521\nDiving Deeper into Data\nMonash University\nThis is an advanced exploratory data analysiis unit, that is based on the spirit of Tukey, to discover the unexpected in data. It has a large focus on making effective plots of data, and checking for suprious structure with modern methods of inference for graphics."
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "Teaching resources",
    "section": "Workshops",
    "text": "Workshops\n\n\n\n\n\n\n\n\ndate\ntitle\nlocation\ndetails\n\n\n\n\n2024-10-22\nCreating effective data plots\nWOMBAT 2024\n3 hours\n\n\n2024-08-14\nSISBID 2024 Module 2: Visualization of Biomedical Big Data\nVirtual\n2.5 days (since 2015)\n\n\n2024-07-08\nVisualising High-dimensional Data with R\nuseR! 2024, Salzburg\n3 hours\n\n\n2024-03-26\nVisualising High-dimensional Data with R\nSSA SCV, online\n3 hours\n\n\n2023-12-02\nCreating data plots for effective decision-making using statistical inference with R\nSSA SCV, online\n3 hours\n\n\n2023-10-19\nCreating data plots for effective decision-making using statistical inference with R\nMontevideo, Uruguay\n3 hours\n\n\n2022-11-28\nVisual methods for multivariate data - a journey beyond 3D\nSASA 2022, George, South Africa\n3 hours\n\n\n2020-06-23\nGoing beyond 2D and 3D to visualise higher dimensions, for ordination, clustering and other models\nvISEC 2020\n3 hours\n\n\n\n\n\n\n\nAnd more than 30 workshops across the globe over many years."
  },
  {
    "objectID": "posts/2015-10-26-CSAFE/index.html",
    "href": "posts/2015-10-26-CSAFE/index.html",
    "title": "Center for Statistics and Applications in Forensic Evidence",
    "section": "",
    "text": "This week I have been visiting the new Center for Statistics and Applications in Forensic Evidence. The center involves four universities, CMU, ISU, UC-Irvine, U. Virginia, and is a NIST Center of Excellence. The kickoff event occurred over Oct 26-27 at ISU, organized by Center Director, Professor Alicia Carriquiry. The speaker list included Barry Scheck (Co-Founder, The Innocence Project), Jo Handelsman (The White House Office of Science & Technology Policy), Philip Dawid (Emeritus Professor of Statistics, University of Cambridge), Anil Jain (Michigan State University) and Stephen Feinberg (CMU). Small discussion groups included these topics shoe prints, firearms forensics, human factors, cyber forensics and pattern evidence. This is the start of potential collaborations between statisticians and forensic scientists to build better quantification of uncertainty in evidence.\nHere is a link to the CSAFE web site."
  },
  {
    "objectID": "posts/2015-10-13-data-science-for-managers/index.html",
    "href": "posts/2015-10-13-data-science-for-managers/index.html",
    "title": "Data Science for Managers, 2015, Monash Conference Center, Melbourne",
    "section": "",
    "text": "Spent a couple of hours this morning talking at the http://it.monash.edu/data-science workshop organised by Michael Brand from Monash University. Good questions, good discussion.\nHere is a link to my slides."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html",
    "href": "posts/2024-08-26-what-is-EDA/index.html",
    "title": "What is EDA, really?",
    "section": "",
    "text": "I’ve been meaning to write about this for several years, but Joe Cheng’s talk at posit::conf on Aug 14, 2024, precipitated the motivation to do it now. His talk slides are at https://speakerdeck.com/jcheng5/ and the topic focuses on using AI for dashboard data analysis.\nThe crux of the talk is a Shiny app focused on the tips data, with a chatbot in the side bar that can change some options or answer other questions. You can play with the app at https://jcheng.shinyapps.io/sidebot/."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#joe-chengs-talk-at-positconf",
    "href": "posts/2024-08-26-what-is-EDA/index.html#joe-chengs-talk-at-positconf",
    "title": "What is EDA, really?",
    "section": "",
    "text": "I’ve been meaning to write about this for several years, but Joe Cheng’s talk at posit::conf on Aug 14, 2024, precipitated the motivation to do it now. His talk slides are at https://speakerdeck.com/jcheng5/ and the topic focuses on using AI for dashboard data analysis.\nThe crux of the talk is a Shiny app focused on the tips data, with a chatbot in the side bar that can change some options or answer other questions. You can play with the app at https://jcheng.shinyapps.io/sidebot/."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#where-did-the-tips-data-come-from",
    "href": "posts/2024-08-26-what-is-EDA/index.html#where-did-the-tips-data-come-from",
    "title": "What is EDA, really?",
    "section": "Where did the tips data come from?",
    "text": "Where did the tips data come from?\nFirst, I have to start with the tips data. This data is special to me. As a beginning lecturer at Iowa State University in 1993 I was given the task of teaching a data analysis (read mostly linear regression) class to business students. So I hunted around to find new data sets to use, ones other than those in “Data” by Andrews and Herzeberg which was the common go-to data source at the time.\nThe tips data comes from Bryant & Smith (1994) “Practical Data Analysis: Case Studies in Business Statistics”, which explained how to determine which factors affect tipping rate from data collected by one waiter at one restaurant for about three months. Tipping immediately drew my attention because being an Australian in the USA, how tipping worked was still confusing to me even after five years of participating in the practice. The case study focused solely on the regression model, but my exploration of the data revealed it had many more interesting patterns. We used this data in the introduction of Cook and Swayne (2007) to explain how exploring data differs from confirmatory analysis."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#what-do-people-think-eda-is",
    "href": "posts/2024-08-26-what-is-EDA/index.html#what-do-people-think-eda-is",
    "title": "What is EDA, really?",
    "section": "What do people think EDA is?",
    "text": "What do people think EDA is?\nToday, exploratory data analysis has been conflated with descriptive statistics. If you read through the resource-rich “The Landscape of R Packages for Automated Exploratory Data Analysis” by Staniak and Biecek, you will find a comparison of numerous R packages reportedly doing EDA, but in reality only do PURELY descriptive statistics."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#eda-is-really-this",
    "href": "posts/2024-08-26-what-is-EDA/index.html#eda-is-really-this",
    "title": "What is EDA, really?",
    "section": "EDA is really this",
    "text": "EDA is really this\n“Exploratory data analysis” was a term coined by John W. Tukey, and marked by the publication of his book with this name in 1977. Interestingly, the attention that this effort received was not appreciated by many in the British school of data analysis, where what Tukey was doing was considered just what many of them had been doing for eons, initial data analysis in the style of Chatfield (1991). However, this was misguided – EDA is very different from IDA.\nI picked up Tukey’s book in 1993, particularly after some colleagues referred to me as the new Tukey, but I could not wrap my head around the content. I was familiar with exploring high-dimensional data with interactive and dynamic graphics, finding new structures in the PRIM-9 data that were not clear that Tukey had seen, not doing pencil-and-paper calculations and plots. In truth, the problem was most likely the style of the writing, as noted in a 1979 review by Ehrenberg who says Tukey’s expository style makes it impossible to dip into. It’s a massive collection of bits and pieces and what might be done with a data set.\nBut the spirit of EDA is clear in the introduction. EDA is the joy of working with data with a clear demand for pictures drawn from data that force us to notice what we never expected to see. Descriptive statistics are boring and dull and extensive. EDA is anything but that – it is exciting and dizzying and thoughtful.\nSlides 19-53 from week 1 of my unit at Monash University ETC5521 illustrate the difference between confirmatory and exploratory data analysis using the tips data. All of the really interesting aspects of this data are found from the exploratory data plots, not from the confirmatory analysis.\n\n\n\nThere is almost no relationship between tip and bill in the subset smoking=Yes."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#where-does-ai-fall-down-on-eda",
    "href": "posts/2024-08-26-what-is-EDA/index.html#where-does-ai-fall-down-on-eda",
    "title": "What is EDA, really?",
    "section": "Where does AI fall down on EDA?",
    "text": "Where does AI fall down on EDA?\nData analysis trends have a cyclical or maybe spiraling nature. Tukey’s EDA was a reaction against the modern obsession with statistical inference. The development of statistical inference architecture was a reaction to informal data analysis, that evolved it into a more organised state. Tukey wanted us not to forget that informal methods could reveal things that were missed by the blinkered constraints of the formal process of statistical inference. Complex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and it returns to a data-centric perspective. AI doesn’t quite fit this data analysis cycle, because it is more related to managing the vast quantities of information stored about our world, and improving the human interface to this information. It should be noted that Tukey was also at the forefront of developing computational methods, that today enable statistical inference to be conducted on complex data and models.\nThe current state of AI for data analysis leaves most of the interesting findings behind the door. The human language interface is definitely better though: as Joe’s chatbot confirms one can ask the question in plain English (hopefully other languages as well), without the labour of turning it into an SQL query or a computer-interpretable line of code.\nThe packages listed in Staniak and Biecek’s article, summarytools, SmartEDA and DataExplorer remove some of the hard work of data analyis, by automatically making extensive numerical and graphical summaries of the columns (hopefully variables) in the data. However, the difficulty in automating this can be seen by the poor quality of graphics, choices of what binwidth for the histograms, long axis labels, poor choices in type of plot and aspect ratio of plots, … There is still hope that this might be improved, especially with the clarity provided by the tidy data principles and the grammar of graphics for plot specification. This helps in handling variable type from a computational perspective, but still misses the statistical perspective (e.g. response vs independent), and it is still hard to decide on appropriate axis scales and aspect ratio, and even real estate on a page needed for reading a plot.\nBut, let’s move on and play with Joe’s dashboard on the tips data, to see whether the interesting details of the data can be divulged. The choice of items in the dashboard are three summary numbers, a full spreadsheet of the data, a scatterplot, and a ridge plot. None of these shows anything interesting. To find the interesting patterns, one needs to: (1) be able to use different binwidths on any histogram or density display, (2) examine subsets by conditioning on the categorical variables.\nNevertheless, the chatbot’s first message is:\nYou can use this sidebar to filter and sort the data based on the columns available in the tips table.\nand it suggests a few possibilities, all of which when asked will change the data shown in the dashboard, but nothing interesting in the data emerges.\nSo let’s ask:\n\nIs there anything interesting in this data?\n\nI don’t have direct access to visualize or browse the data myself…\nThat was wishful thinking, like the student who dumped my first assignment fully into the AI system hoping for an easy solution. So let’s try something where I know there should be an interesting pattern revealed.\n\nWould you facet by smoker and sex?\n\nSince you want to facet, I will filter the dashboard to show all columns but segregate the data based on these categorical values.\nand it shows the SQL query string, but doesn’t re-do the summaries for these subsets. That’s a human issue, because the dashboard has not been designed to enable this type of user choice.\nWe can ask this more generally with\n\nIs the relationship between tip and total bill different for by smoking and sex?\n\nLet’s calculate the correlation coefficient between tip and total_bill for each combination of smoker status and sex.\nwhich makes a nice summary of the conditional correlations and does show much lower correlation for the smoking parties. Let’s go for another question where I know the answer should reveal something interesting about tipping behaviour:\n\nIs there some piling of tips at certain values?\n\nSpecifically, we can count the frequency of each unique tip value.\nand it provides the top counts, which I can see are at whole dollar amounts, and it also gives a nice summary statement:\nFrom the results, it is clear there is some piling or clustering of tips around certain values, particularly at $2.00, $3.00, $4.00, and $5.00. This suggests that when tipping, people tend to round to whole numbers or common fractional values like $0.50.\nOn the whole the chatbot response is a summary that uses single numerical scores, means, standard deviation, correlation. If the dashboard design had a slightly more sophisticated display, that allowed facetting or subsetting, and had a reasonable way to handle the complications that this induces, the chatbot might be more effective. So, if the scatterplot of tip vs total bill could be facetted by another variable or two, then the chatbot might have been able to control this. The problem of facetting though, produces a dilemma of real estate in the dashboard, four plots might not be readable in the space of a single plot. A solution to this is to enable re-plotting, cycling through the four plots instead of all displayed together.\nDashboard designs that cleverly integrate these sorts of interactivity, plot cycling, active selection, linked information between plots, are needed. Dashboards that can also show the user what this plot (or numerical summary) might look like if there really was nothing interesting, as done with lineups using the nullabor package, would be supercalifragilisticexpialidocious, and truly capture the spirit of Tukey’s EDA."
  },
  {
    "objectID": "posts/2024-08-26-what-is-EDA/index.html#so-what",
    "href": "posts/2024-08-26-what-is-EDA/index.html#so-what",
    "title": "What is EDA, really?",
    "section": "So what?",
    "text": "So what?\nProbably, this post is not going to change the current conflation of terms. And my resignation to this was to rename my unit from “Exploratory Data Analysis” into “Diving Deeper into Data Exploration”.\nHowever, I do hope this post inspires you to get beyond descriptive statistics, to go out and explore to find what you never expected to see (from data) with creative use of statistical graphics and visual inference, and to endeavour to design lit dashboards."
  },
  {
    "objectID": "posts/2014-10-5-statistical-computing/index.html",
    "href": "posts/2014-10-5-statistical-computing/index.html",
    "title": "Statistical computing research",
    "section": "",
    "text": "During the week, I received final confirmation notice that the special issue of Statistical Science that Vince Carey and I put together is finally published. There are four papers from leaders in the field of statistical computing research: John Chambers, Duncan Temple Lang, Michael Lawrence and Michael Morgan (newly minted members of R Core) and Yihui Xie, Heike Hofmann and Xiaoyue Cheng. The links to the overview and the four papers are below.\n\nFour Papers on Contemporary Software Design Strategies for Statistical Methodologists Vincent Carey and Dianne Cook\nObject-Oriented Programming, Functional Programming and R John Chambers\nScalable Genomics with R and Bioconductor Michael Lawrence and Martin Morgan\nEnhancing R with Advanced Compilation Tools and Methods Duncan Temple Lang\nReactive Programming for Interactive Graphics Yihui Xie, Heike Hofmann and Xiaoyue Cheng\n\nThese four papers document statistical computing research, and hopefully inspire talented young researchers to tackle ideas in these areas.\n“Statistical computing” is different to “computational statistics”. Computational statistics research develops of algorithms for statistical analysis, eg project pursuit, importance sampling, maximum likelihood. Statistical computing research explores the availability of new technology for statistical purposes. There has been a long history of statistical computing research dating back, from my knowledge, to at least the 1960s. When computing languages were first being developed John Chambers, Rick Becker and Allan Wilks started thinking about how a language might support data analysis, and thus was the birth of the S language, which is used today in R. Statistical computing researchers have brought us systems such as S, Splus, R, SAS, SPSS, Systat, XLispStat, XploRe, DataDesk, GENSTAT, Minitab, Antelope, Dataviewer, QUAIL, XGobi, GGobi, Mondrian, Orca. I have missed a lot, I am sure, so if you know of more, please add them in comments at the end of the blog.\nMore than a decade ago, I documented a discussion between four attendees at a workshop on the future of statistical computing in the newsletter of the ASA Sections of Statistical Computing and Graphics, which can be found here. (BTW, If you browse these newsletters look for some real gems in the short articles, particularly many by Dan Carr on elegant data visualizations. This newsetter has largely been supplanted by the R Journal today.) There was concern about the future of statistical computing research in the face of declining support for these areas in industry research labs. Substantial accomplishments have historically come from places like Bell Labs. The big question was whether the research support could make the leap from these sources into academia.\nAlthough, it is is still not easy, to a large extent statistical computing research has made the leap. The authors of three of these four papers are in academic positions. In numerous departments around the world, statistical computing research is thriving. Some examples are University of Auckland, University of Waterloo, Augsburg University, Harvard Biostats (please add more in the comments!). From personal experience, we have had steady support from the administration at Iowa State University in promoting statistical computing research, especially focusing on graphics. In addition, new homes in industry have emerged, like RStudio which is a hot-bed of open-source development, and Genentech and FHCRC have brought us bioconductor.\nWith the growing importance of data science, to tackle big data problems, statistical computing is a critical area for the Statistics research."
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "",
    "text": "I just gave a short talk at ISCB-ASC 2018 about visualising high-dimensional data, which involves showing dynamic graphics. In the past, I have run the tour, captured the window and saved to a movie, and embedded this into the Rmarkdown xaringan slides. It seems a bit discombobulated to make the slides this way, and a better way to work would be to make a tour animation using plotly. This turned out to take me two days to get it working, through little mistakes that were not easy to debug by googling the problem. This blog post documents the process, and hopefully shows the little frustrations and how to get past them."
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#goal",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#goal",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "",
    "text": "I just gave a short talk at ISCB-ASC 2018 about visualising high-dimensional data, which involves showing dynamic graphics. In the past, I have run the tour, captured the window and saved to a movie, and embedded this into the Rmarkdown xaringan slides. It seems a bit discombobulated to make the slides this way, and a better way to work would be to make a tour animation using plotly. This turned out to take me two days to get it working, through little mistakes that were not easy to debug by googling the problem. This blog post documents the process, and hopefully shows the little frustrations and how to get past them."
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#tour",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#tour",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "Tour",
    "text": "Tour\nThe tourr package, elegantly crafted by Hadley Wickham, provides a broad range of tour types, and is easy to run locally on your laptop. It doesn’t play very nicely with the RStudio graphics device, so requires opening a separate device which is easy:\nquartz() # OSX, use X11() on Windows or linux\nlibrary(tourr)\nanimate_dist(flea[, 1:6])\nanimate_scatmat(flea[, 1:6], grand_tour(6))\nanimate_pcp(flea[, 1:6], grand_tour(3))\nanimate_faces(flea[sort(sample(1:74, 4)), 1:6], grand_tour(4))\nanimate_stars(flea[sort(sample(1:74, 16)), 1:6], grand_tour(5))"
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#creating-recorded-tour-with-plotly",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#creating-recorded-tour-with-plotly",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "Creating recorded tour with plotly",
    "text": "Creating recorded tour with plotly\nThe save_history function generates a sequence of projection bases that can be used to construct a tour, with any other tools.\nlibrary(tidyverse)\nlibrary(tourr)\nbases &lt;- save_history(flea[,1:6], grand_tour(2), max = 3)\ntour_path &lt;- interpolate(bases, 0.1)\nCreating a plotly animation requires one data structure containing all the data, with a variable indicating the animation frame. This is the ugly part of the code! Maybe its just my way of making this, there are sure to be nicer ways to do this. The code loops over the number of projection planes, does a matrix multiplication with the data, adds the frame index column. The index starts from 10, because when I started from 1 plotly messed up the order.\nflea_std &lt;- apply(flea[,1:6], 2, function(x) (x-mean(x))/sd(x))\nd &lt;- dim(tour_path)\nflea_anim &lt;- NULL\nfor (i in 1:d[3]) {\n  f &lt;- flea_std %*% matrix(tour_path[,,i], ncol=2)\n  colnames(f) &lt;- c(\"x\", \"y\")\n  flea_anim &lt;- rbind(flea_anim, cbind(f, rep(i+10, nrow(f))))\n}\ncolnames(flea_anim)[3] &lt;- \"indx\"\nflea_anim &lt;- as.tibble(flea_anim)\nflea_anim$species &lt;- rep(flea$species, d[3])\nThis can now be played using ggplotly, after first plotting using ggplot2. Note the aes(frame = indx) which throws a warning from ggplot, is essential for making the animation.\np &lt;- ggplot(data = flea_anim, aes(x = x, y = y, colour=species) ) +\n       geom_point(aes(frame = indx), size=1) +\n       theme_void() +\n       coord_fixed()\nlibrary(plotly)\npg &lt;- ggplotly(p, width=600, height=400) %&gt;%\n                 animation_opts(200, redraw = FALSE, \n                 easing = \"linear\")\npg\nOne change I’d like to have in plotly, but I couldn’t see how to do this, is easing completely off. The tour controls the interpolation and doesn’t need any additional interpolation. (CORRECTION: use transition=0 to achieve this!)"
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#summary",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#summary",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "Summary",
    "text": "Summary\nThe old approach can be seen in the code for slides for IMS Singapore. It is prettier code, despite the cognitive break of showing a movie.\nIt took me several instant messaging with Carson Sievert to get it working. Carson’s advanced plotly slides and code explain the process in detail, but the two things that tripped me up, saving to html and the sizing are not documented there. Carson also recommends Bhaskar Karambelkar’s widgetframe. Code for my presentation is here."
  },
  {
    "objectID": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#sources",
    "href": "posts/2018-08-29-getting-past-the-little-hiccups-to-getting-plotly-animations-into-slides/index.html#sources",
    "title": "Getting past the little hiccups to getting plotly animations into slides",
    "section": "Sources",
    "text": "Sources\nThe code for this post is here."
  },
  {
    "objectID": "posts/2014-09-25-facetted-barcharts/index.html",
    "href": "posts/2014-09-25-facetted-barcharts/index.html",
    "title": "Facetted barcharts, and fluctuation diagrams are good alternatives to stacked barcharts",
    "section": "",
    "text": "When there are two categorical variables it is common to make a stacked barchart. The stacked barchart primarily allows the reader to see the overall count, but it is harder to compare the counts of categories, the colored segments. Using data from the vcd package in R, here is an example. The data describes the responses of couples on questions about their sex life.\nThis is a bar chart showing the husbands views, with his wifes’ views forming the stacking. The primary message is that mostly husbands say that Sex is “Always Fun”. Separately, we can see that the color strips are not of equal length in each bar, which would indicated some difference with the wife’s viewpoint, but it takes some work to dig in deeper to digest how they are different.\n\n\n\n“stacked bar 1”\n\n\nSame data, arranged differently. The bars represent the wifes’ views, and the husbands’ responses forming the stacking. The distribution of wifes’ responses is different to the husbands (previous barchart) - wives respond with “Fairly Often” more frequently than husbands.\n\n\n\n“stacked bar 2”\n\n\nIt would also be common to show these as 100% charts, to read off the proportions. Showing both the barchart and the 100% chart enables viewing the overall frequencies, and the proportions within each category. It requires that the reader shift back and forward between the two plots to take both requency and proportion into account, which is hard work.\nOne alternative is to create separate bar charts for each response category for the other variable, as done below. The distribution of frequencies in each category are now easy to read (but overall count is more difficult). The main message is that the distribution of husbands’ responses is different depending on the wifes’ responses. When the wife responds “Never Fun” the husband typically also responds “Never Fun”. If the wife responds “Always Fun” the husband tends to respond “Always Fun” too. When the wife responds “Fairly Often” the husbands’ responses are uniform across the four possible answers.\n\n\n\n“stacked bar 1”\n\n\nRearranging in the opposite order changes the focus to wifes’ responses given the husbands’ responses.\n\n\n\n“stacked bar 2”\n\n\nUsing both arrangements we can the asymmetry between each member of the couples responses.\nTo better read the symmetry, or asymmetry, show the data as a fluctuation diagram. Here the boxes reflect the count for the combination of the categories. The main message is that the couples agree, roughly, on the perception of sex (bigger boxes on leading diagonal), and there is asymmetry in the couples separate responses (boxes above and below diagonal are not equal size).\n\n\n\n“fluctuation”\n\n\nIn summary, it is recommended to do a suite of plots rather than one single plot for problems like this, with the suggested set being:\n\noverall frequency for each variable using a single barchart for each variable\nfacetted bar chart, arranged in two ways\nfluctuation diagram\n\nSee Unwin et al for more information about creating suites of plots to display data."
  },
  {
    "objectID": "posts/2018-04-22-smart-meter/index.html",
    "href": "posts/2018-04-22-smart-meter/index.html",
    "title": "Analysing my energy usage",
    "section": "",
    "text": "You can get access to your own electricity and gas usage data from https://www.citipower.com.au/our-services/myenergy. You will need a copy of your power bill, which has your smart meter number and meter id, to register for an account."
  },
  {
    "objectID": "posts/2018-04-22-smart-meter/index.html#download-your-data",
    "href": "posts/2018-04-22-smart-meter/index.html#download-your-data",
    "title": "Analysing my energy usage",
    "section": "",
    "text": "You can get access to your own electricity and gas usage data from https://www.citipower.com.au/our-services/myenergy. You will need a copy of your power bill, which has your smart meter number and meter id, to register for an account."
  },
  {
    "objectID": "posts/2018-04-22-smart-meter/index.html#reading-the-data",
    "href": "posts/2018-04-22-smart-meter/index.html#reading-the-data",
    "title": "Analysing my energy usage",
    "section": "Reading the data",
    "text": "Reading the data\nThe data structure is described here.\nThe data is not especially nicely formatted (surprise). The main components are:\n\nThe time resolution is half-hourly. And values for each day are spread across the columns.\nThe first column has an id variable which indicates what type of information is in the row. If it is “200” is indicates a new meter. You may have multiple meters running, and hence multiple recordings per day. Only id’s equal to “300” are actually measurements.\n\nThe code to read the data does the following steps:\n\nSkip the first row\nCheck the number of meters, create a meter id and day index for each meter\nFilter only rows with id “300”\nGather the data into long form, to create a variable indicating the half hourly intervals, and the kwh in each.\nCreate some new time variables, day, month, year."
  },
  {
    "objectID": "posts/2018-04-22-smart-meter/index.html#sources",
    "href": "posts/2018-04-22-smart-meter/index.html#sources",
    "title": "Analysing my energy usage",
    "section": "Sources",
    "text": "Sources\nCode and data"
  },
  {
    "objectID": "posts/2022-05-22-election_hexmaps/index.html",
    "href": "posts/2022-05-22-election_hexmaps/index.html",
    "title": "Hexmaps with sugarbag make it easier to see the electoral map",
    "section": "",
    "text": "Australia is a land of wide open spaces where the population concentrates in small areas. It can make for misleading map visualisations on statistics related to people. The May 20, 2022 ABC article The Australian election map has been lying to you explains this very neatly. It has alsp provided a better alternative to examine election results, in the form of a hexmap of Australia. The hexmap provided in the article is almost certainly manually constructed which is find for a construct once, use many times purpose.\nWhen you want to be able to make a hexmap on new spatial data or if the spatial groups change, the R package sugarbag can be helpful. This post explains how to do this, using the results as we have them today from yesterday’s election. (We’ll update these once the final results are released.)\nHere’s how to get started. Download the current spatial boundaries for electorates, from Australian Electoral Commission web site.\nLoad the libraries we need:\n\nlibrary(ggthemes)\nlibrary(sf)\nlibrary(sugarbag)\nlibrary(tidyverse)\nlibrary(plotly)\n\nRead in the spatial polygons, defining the boundaries. These files can be very large, and slow to draw. For these visualisations faster to draw is more important, so the boundaries can be simplified using rmapshaper::ms_simplify.\n\n# Spatial polygons\nelectorates &lt;- sf::st_read(\"2021-Cwlth_electoral_boundaries_ESRI/2021_ELB_region.shp\")\nelectorates_small &lt;- electorates %&gt;% rmapshaper::ms_simplify(keep = 0.01, keep_shapes = TRUE)\n\nNext we need the elction results. The ones here are manually constructed from the ABC results website. These results are joined to the map polygons, and colours are manually constructed to be one typically used by the party. The ggplotly() function enables labels to pop up on mouseover.\n\n# Read in data on current electoral results\nnew &lt;- read_csv(\"electoral_2022.csv\") %&gt;%\n  select(Electorate:Party)\nnew_major &lt;- new %&gt;%\n  mutate(Party_maj = fct_collapse(Party,\n         LNP = c(\"LIB\", \"LNP\", \"NAT\")))\nelectorates_small &lt;- electorates_small %&gt;%\n  left_join(new_major, by=c(\"Elect_div\"=\"Electorate\"))\nmap &lt;- ggplot() +\n  geom_sf(data=electorates_small,\n                   aes(fill = Party_maj,\n                       label=Elect_div),\n                       colour=\"white\") +\n  scale_fill_manual(\"\", values=c(\"ALP\"=\"#E13940\",\n                                   \"LNP\"=\"#1C4F9C\",\n                                   \"GRN\"=\"#009C3D\",\n                                   \"KAP\"=\"#906E3E\",\n                                   \"CA\"=\"#FFC000\",\n                                   \"IND\"=\"#66b2b2\",\n                                   \"UNDEC\"=\"#808080\")) +\n  theme_map()\nmap\nggplotly(map)\n\n\n\nThe map is blue – it looks like the coalition won the election in a landslide, doesn’t it! (Please note the strange shape of the Cape of York is from the AEC spatial polygons provided! It is not due the the polygon thinning.)\nTo convert this into a hexmap, automatically with sugarbag, we need to\n\nFind the centroids of each polygon.\nCreate a hexagon grid with a desired size of hexagon, hs controls this.\nAllocate electorates to a spot on the grid.\nTurn the hexagon centroids into hexagons.\nJoin with election results.\nMake it interactive using ggplotly().\n\n\n# Find centroids of polygons\nsf_use_s2(FALSE)\ncentroids &lt;- electorates %&gt;%\n  create_centroids(., \"Elect_div\")\n\n## Create hexagon grid\nhs &lt;- 0.8\ngrid &lt;- create_grid(centroids = centroids,\n                    hex_size = hs,\n                    buffer_dist = 5)\n\n## Allocate polygon centroids to hexagon grid points\nelectorate_hexmap &lt;- allocate(\n  centroids = centroids,\n  hex_grid = grid,\n  sf_id = \"Elect_div\",\n  ## same column used in create_centroids\n  hex_size = hs,\n  ## same size used in create_grid\n  hex_filter = 10,\n  focal_points = capital_cities,\n  width = 35,\n  verbose = FALSE\n)\n\n# Make the hexagons\ne_hex &lt;- fortify_hexagon(data = electorate_hexmap,\n                            sf_id = \"Elect_div\",\n                            hex_size = hs)  \nelectorate_hexmap_new &lt;- e_hex %&gt;%\n  left_join(new_major, by=c(\"Elect_div\"=\"Electorate\"))\nhexmap &lt;- ggplot() +\n  geom_sf(data=electorates_small, \n          fill=\"grey90\", colour=\"white\") +\n  geom_polygon(data=electorate_hexmap_new,\n             aes(x=long, y=lat,\n                 group = hex_id,\n                 fill=Party_maj,\n                 label=Elect_div)) +\n  scale_fill_manual(\"\", values=c(\"ALP\"=\"#E13940\",\n                                 \"LNP\"=\"#1C4F9C\",\n                                 \"GRN\"=\"#009C3D\",\n                                 \"KAP\"=\"#906E3E\",\n                                 \"CA\"=\"#FFC000\",\n                                 \"IND\"=\"#66b2b2\",\n                                 \"UNDEC\"=\"#808080\")) +\n  theme_map()\nhexmap\nggplotly(hexmap)\n\n\n\nAnd that’s it! The sugarbag hexmap will expand the densely populated small areas outwards, while maintaining proximity to neighbouring electorates and to the city centre. It is a type of cartogram algorithm with two important differences: (1) uses equal area for each hexagon instead of sized proportional to population, and (2) allows some hexagons to be separated so that the geographic positions are reasonably preserved.\nThe hexmap makes it easier to see the results distributed across the country, and clearly with the predominance of red, that Labor won.\nData for this post can be found here.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/2014-09-13-exploring-gradebook/index.html",
    "href": "posts/2014-09-13-exploring-gradebook/index.html",
    "title": "A Graphical Expedition into a Statistics Gradebook",
    "section": "",
    "text": "It is always with a sense of unease that I reduce a whole semester’s work into a single letter grade, and to alleviate this feeling, I often pack the gradebook into an interactive graphics system like ggobi or cranvas, and perambulate over it. In the second issue of Chance 2014, I wrote about doing this on grades for a large introductory statistics class using interactive graphics. The class had on the order of 100 students, and grades from exams, homeworks, labs, worksheets, online quizzes and a data analysis project.\nRead more\nVideos of the exploration are available at:\n\nExploring exam scores\nTouring exam scores\nClustering exam scores\nAll scores"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Invited talks and seminars",
    "section": "",
    "text": "date\ntitle\nlocation\ndetails\n\n\n\n\n2025-04-24\nInteractively Visualizing Multivariate Market Segmentation Using the R Package Lionfish\nSymposium in Memory of Fritz Leisch\nInvited talk\n\n\n2025-03-28\nVisually exploring high-dimensional market segmentation data\nDAGStat 2025\nInvited talk\n\n\n2024-11-07\nNew tools for visualising high-dimensional data using linear projections\nTDLee Institute, Shanghai, China\ninvited talk\n\n\n2024-10-27\nInteractively exploring market segmentation with high-dimensional visualisation\nMPSS, Shanghai, China\ninvited talk\n\n\n2024-03-13\nA little history of GGally\nOnline, ggplot2 extenders\ninvited talk\n\n\n2023-12-12\nVisually exploring local explanations to understand complex machine learning models\nASC and OZCOTS 2023\ncontributed\n\n\n2023-12-07\nNew tools for visualising high-dimensional data using linear projections\nIASC ARS 2023\nkeynote\n\n\n2023-10-20\nHow do we know what we don’t know?\nMontevideo, Uruguay\nkeynote\n\n\n2022-11-24\nVisually exploring local explanations to understand complex machine learning models\nStellenbosch U., South Africa\ninvited talk\n\n\n2022-08-29\nMyth busting and apophenia in data visualisation: Is what you see really there?\nMacquarie U., Australia\ninvited talk\n\n\n2022-07-20\nA showcase of new methods for high dimensional data viewing with linear projections and sections\nIFCS '22, Porto, Portugal\nkeynote\n\n\n2021-06-21\nUncertainty in data visualisation through the lens of statistical inference\nRostock Retreat on Data Visualization at the Max Planck Institute for Demographic Research\nkeynote\n\n\n2021-02-03\nIn Visualising Data, Who Wins?\nGosset lecture, Irish Statistical Association\nnamed lecture\n\n\n2020-07-20\nThe paradox of the positive: exploratory tools for visualising the individuals in (multivariate) longitudinal data\nIBC 2020\nkeynote\n\n\n2020-06-24\nMaking inference using data plots, with application to ecological statistics\nvISEC 2020\nkeynote\n\n\n2019-09-18\nStatistics on Street Corners\nCSIRO Data61, Canberra, Australia\ninvited talk\n\n\n2019-08-20\nBuilding plots to explore data\n62nd ISI World Statistics Congress, Kuala Lumpur, Malaysia\ncontributed\n\n\n2019-08-14\nHuman vs computer: In Visualising Data, Who Wins?\nDSSV '19, Kyoto, Japan\nkeynote\n\n\n2019-07-30\nGive Your Statistician Colleague Iris Bulbs for Their House Warming!\nJSM 2019, Denver, Colorado\ninvited talk\n\n\n2018-10-15\nHuman vs computer: when visualising data, who wins?\nBelz Lecture, Statistical Society of Victoria\nnamed lecture\n\n\n2018-09-12\nVisualisation of high-dimensional spaces with application to econometric data and models\nU. Melbourne, Melbourne, Australia\ninvited talk\n\n\n2018-08-28\nVisualisation of high-dimensional particle physics\nIASCB-ASC 2018, Melbourne, Australia\ncontributed\n\n\n2018-07-30\nEDA: A Historical Perspective and a Path Forward\nJSM 2018, Vancouver, Canada\ninvited talk\n\n\n2018-03-07\nMyth busting and apophenia in data visualisation: is what you see really there?\nIhaka Lecture, University of Auckland, NZ\nnamed lecture\n\n\n2018-02-12\nVisualising high-dimensional spaces with application to particle physics models\nNUS/IMS Workshop and Tutorial on Social Networks, Singapore\ninvited lecture\n\n\n2018-02-02\nTo the Tidyverse and Beyond: Challenges for the Future in Data Science\nRStudio conference, San Diego, CA\nkeynote\n\n\n2017-11-27\nStatistics on Street Corners\nBiometrics by the Border, Kingscliff, NSW\nkeynote\n\n\n2017-09-26\nStatistics on Street Corners\nYoung Statisticians Conference 2017, Coolangatta, QLD\nkeynote\n\n\n2017-07-29\nA girl geek’s guide to new research on interactive data visualization for statistics with lots of data.\nJSM '17, Baltimore, MD\ninvited talk\n\n\n2017-05-19\nThe Glue that Binds Statistical Inference, Tidy Data, Grammar of Graphics, Visual Inference\nSchool of Maths and Stats, U. Sydney,\ninvited talk\n\n\n2017-05-18\nThe Glue that Binds Statistical Inference, Tidy Data, Grammar of Graphics, Visual Inference\nFinance, Actuarial Studies & Statistics, Australian National University, Canberra.\ninvited talk\n\n\n2017-05-03\nAccessing data and developing analysis software, to study game strategies, with examples from tennis and soccer\nACEMS Workshop: Maths and Stats for Sports Research, Brisbane, Australia.\nNA\n\n\n2016-11-28\nStatistics on Street Corners\n2016 Joint New Zealand Statistical Association/Operations Research Society of New Zealand Conference, Auckland, New Zealand\nkeynote\n\n\n2016-11-25\nThe Role of Open Data, Open Source Software and Data Visualisation in Developing Quantitative Citizenship\nAuckland Teachers Day, New Zealand\ninvited talk\n\n\n2016-10-27\nHarnessing Crowd-Sourcing to Select Genes based on Effect Size Using Visual Inference Methods\nVic Biostat, Melbourne, Australia\ninvited talk\n\n\n2016-10-20\nTaskforce on Women in R\nWomen in Statistics, Charlotte, NC\ninvited talk\n\n\n2016-10-18\nThe Role of R and Data Visualisation in Understanding Our World\nR Ladies, Melbourne, Australia\ninvited talk\n\n\n2016-05-28\nStatistics on Street Corners\nInternational Statistics Forum, Beijing, China\nkeynote\n\n\n2016-05-27\nReally? Using the nullabor package to learn if what we see is really there?\nChina-R, Capital of Statistics, Beijing, China\ninvited talk\n\n\n2016-03-15\nIs what you see really there? Combining statistical inference with exploratory data analysis\nUniversity of Melbourne, Australi\ninvited talk\n\n\n2016-02-19\nReally? Using the nullabor package to learn if what we see is really there?\nWOMBAT 2016, Melbourne, Australia\ninvited talk\n\n\n2015-12-08\nIs what you see really there? Combining statistical inference with exploratory data analysis\nBayes on the Beach, Surfers Paradise, Australia\nkeynote\n\n\n2015-12-02\nVisualisation in Big Data Analysis: Constructing useful plots, using interactivity, incorporating inference\nResearch School of Finance, Actuarial Studies and Applied Statistics Summer Camp, Murramurang, Australia\nplenary\n\n\n2015-11-04\nStatistical Inference by Crowd-Sourcing\nCornell University, Ithaca, NY\ninvited talk\n\n\n2015-09-24\nIs what you see really there?\nBig Data Visual Analytics '15, Hobart, Australia\nkeynote\n\n\n2015-07-02\nA survey of two decades of efforts to build interactive graphics capacity in R\nuseR! '15. Aalborg, Denmark\nkeynote\n\n\n2015-06-29\nEye-balling is OK again: Using an eye-tracker to see how people read data plots\nData meets Vis: Interactions and Interactivity, Augsburg, Germany\ninvited talk\n\n\n2015-06-17\nData Visualization and Statistical Graphics in Big Data Analysis\nStatistical Inference, Learning and Models for Big Data, Boot Camp, Fields Institute, Toronto, Canada\ninvited talk\n\n\n2014-08-03\nHarnessing Crowd-Sourcing to Select Genes based on Effect Size Using Visual Inference Methods\nJSM '14, Boston, MA.\ninvited talk\n\n\n2014-07-11\nStatistical Inference by Crowd-Sourcing\nASC, Sydney, Australia\ninvited talk\n\n\n2014-04-21\nStatistical Inference by Crowd-Sourcing\nMontana State University, Bozeman, MT\ninvited talk\n\n\n2014-02-18\nStatistical Inference by Crowd-Sourcing\nMichigan State University, East Lansing, MI\ninvited talk\n\n\n2013-08-04\nBig Data and R, Discussant\nJSM '13, Montreal, Canada\ninvited talk\n\n\n2013-06-10\nRecent Advances in Visualization Techniques\nNCSES Digital SEQ, Washington, DC\ninvited talk\n\n\n2012-10-18\nStatistical Inference for Data Visualization\nCarl Morris Honorary Symposium, Washington D. C.\nkeynote\n\n\n2012-06-19\nEvery Plot Must Tell a Story\nuseR! 2012, Nashville, TN\nkeynote\n\n\n2012-05-16\nggbio: Extending the Grammar of Graphics to Genomic Data\nInterface Between Statistics and Computing Science, Houston, T\ninvited talk\n\n\n2012-02-20\nAn Overview of Statistical Inference\nDagstuhl Information Visualization, Visual Data Mining and Machine Learning,Dagstuhl, Germany\ninvited talk\n\n\n2012-02-13\nOverview of Biological Visualization\nISU BCB Seminar Series, Ames, IA\nNA\n\n\n2011-10-20\nShow Me the Data\nChristian Petersen Art Museum, Morrill Hall\nNA\n\n\n2011-08-22\nStatistical Inference for Visual Methods\nInternational Indian Statistical Association, Raleigh, NC\ninvited talk\n\n\n2011-08-02\nIntroductory Overview Lecture: Statistical Graphics\nJSM 2011, Miami, FL\ninvited talk\n\n\n2011-08-01\nVisualizing Climate Change Data\nJSM 2011, Miami, FL\nNA\n\n\n2011-06-03\nExploring Dose Response Data and Repeated Measures Data Using Interactive Graphics\nSCVMMI, Dubuque, IA\ninvited talk\n\n\n2011-02-16\nVisual communication\nCEAH Colloquium, Ames, IA\ninvited talk\n\n\n2010-02-08\nStatistical Inference for Visual Methods\nIowa State University\ninvited talk\n\n\n2009-10-15\nStatistical Inference for Visual Methods\nUniversity of Washington, St Louis\ninvited talk\n\n\n2009-08-18\nUsing R and GGobi to Enhance the Learning of Multivariate Analysis and Data Mining\nISI 2009, Durban, South Africa\ninvited talk\n\n\n2009-07-15\nExploring Variation in Data with Statistical Graphics\nSeminar on Innovative Approaches to Turn Statistics into Knowledge, US Census Bureau, Washington DC\ninvited talk\n\n\n2009-03-27\nExploring Longitudinal Data: A Peek at Workforce Experiences\nSecond Midwest Statistics Research Colloquium, University of Chicago\ninvited talk\n\n\n2009-02-23\nFrom Restaurants to Climate Change: Using Statistics to Understand Our World\nMathematics on the Road Experience, Prairie Lakes AEA\ninvited talk\n\n\n2009-02-10\nStatistical Inference for Visual Methods\nHouston Chapter of the American Statistical Society\ninvited talk\n\n\n2008-06-27\nBrain Candy: Using Data Graphics to Learn about Variability and Uncertainty in Our World\nPCST-10 (Public Communication in Science and Technology, Malmo, Sweden\ninvited talk\n\n\n2008-06-25\nCategorical Variable Linking for Exploring Longitudinal Data\nData Vis VI, Bremen, Germany\ninvited talk\n\n\n2008-05-30\nUsing R and Ggobi for Exploratory Data Analysis\nSeminar in Statistik, ETH, Zurich\ninvited talk\n\n\n2008-05-29\nExploring Longitudinal Data: A Look at Workforce Experiences\nZurcher Kolloquium, ETH, Zurich\ninvited talk\n\n\n2008-04-25\nAn Exploratory Approach to Longitudinal Data Analysis\nDepartment of Statistics, University of Bristol, UK\ninvited talk\n\n\n2008-03-28\nAn Exploratory Approach to Longitudinal Data Analysis\nDepartment of Statistics, University of Glasgow, UK\ninvited talk\n\n\n2008-03-06\nAn Exploratory Approach to Longitudinal Data Analysis\nDepartment of Statistics, Oxford University, UK\ninvited talk\n\n\n2008-01-10\nLooking at Models in High-Dimensional Data Space\nContemporary Frontiers in High-Dimensional Statistical Data Analysis, Cambridge, UK\ninvited talk\n\n\n2007-08-01\nImproving Statistical Posters\nIntroductory Overview Lecture, JSM '07\ninvited talk\n\n\n2007-08-01\nExploring models for clustering data\nJSM '07\ninvited talk\n\n\n2006-12-01\nAn EDA of my CDs\nUniversity of Auckland, New Zealand\ninvited talk\n\n\n2006-05-01\nAn EDA of my CDs\nUniversity of California, Los Angele\ninvited talk\n\n\n2005-10-01\nGraphics for Multivariate Data\nGold Standard(s) for Education conference, Vancouver Island, BC, Canada\ninvited talk\n\n\n2005-03-01\nExploring Microarray Data with Plots\nUniversity of Iowa\ninvited talk\n\n\n2004-07-15\nVisualizing Class and Cluster Structure beyond 3D\nInternational Federation of Classification Societies annual meeting, Chicago, IL\ninvited talk\n\n\n2004-04-22\nVisual Methods for Data from Two Factor Single-Replicate Gene Expression Studies\nUniversity of New South Wales, Sydney, Australia\ninvited talk\n\n\n2004-04-19\nVisual Methods for Data from Two Factor Single-Replicate Gene Expression Studies\nDepartment of Statistics, University of Newcastle, Newcastle, Australia\ninvited talk\n\n\n2004-03-21\nCSIRO, Sydney, Australia\nCSIRO, Sydney, Australia\ninvited talk\n\n\n2004-03-16\nData through the windshield in p-dimensions\nDepartment of Statistics, Macquarie University, Sydney, Australia\ninvited talk\n\n\n2004-02-27\nEDA using Direct Manipulation Graphics\nSydney Summer Statistics Workshop, Sydney University, Australia\ninvited talk\n\n\n2003-09-10\nUsing Graphics in Exploratory Data Analysis and Data Mining: An Application of Supervised Classification in Olive Oil Qualit\nData Mining and Machine Learning workshop, Statistical and Applied Mathematical Science Institute, Research Triangle Park, NC\ninvited talk\n\n\n2003-04-10\nSome Dynamic Graphical Tools to Assist Analysis of Microarray Data\nWorkshop on Microarrays, Victorian Microarray Technology Consortium, Melbourne, Victoria, Australia\nkeynote\n\n\n2002-12-13\nClassification Tours Applied to Microarray Data\nDepartment of Statistics, Harvard University\ninvited talk\n\n\n2002-10-08\nUsing Multimedia Animation with Real-time Graphic Overlays for Visualizing a Million Cases of Multivariate Data\nHow to Visualize a Million, Augsburg, Germany\ninvited talk\n\n\n2002-09-12\nVisualization of Microarrays Using Tours and Projection Pursuit\nCambridge HealthTech Institute Conference, Washington, DC\ninvited talk\n\n\n2002-07-17\nJUnderstanding Support Vector Machine Classifiers using Graphics\nCurrent Advances and Trends in Nonparametric Statistics, Crete, Greece\ninvited talk\n\n\n2002-07-11\nLimn: Using Movie Technology to Drive Graphics for Massive Amounts of Data\nUniversity of Augsburg, Germany\ninvited talk\n\n\n2002-04-09\nDynamic Graphics for Multivariate Space-Time Data\nGeology Department, ISU\ninvited talk\n\n\n2001-12-12\nVisual Data Mining of Large, Multivariate Space-Time Data\nAmerican Geophysical Union San Francisco\ninvited talk\n\n\n2001-10-01\nVisual Data Mining: Interactive Dynamic Graphics for Data Analysis\nDepartment of Statistics, Macquarie university, Australia\ninvited talk\n\n\n2001-07-01\nInteractive and Dynamic Graphics for Data Analysis}\nWorkshop on Statistical Challenges in Modern Astronomy, Penn State University, State College, PN\ninvited talk\n\n\n2001-05-01\nVisual Data Mining of Large, Multivariate Space-Time Data\nDept of Statistics, University of Western Australia\ninvited talk\n\n\n2001-01-01\nLimn, to present an image or lifelike imitation of; Limit-n, to explore any data set to its limits\nWorkshop on Non-parametric Statistics, Southern Methodist University, Dallas, TX\ninvited talk\n\n\n2000-11-01\nImmersed in Statistics: your worst nightmare or your wildest dream!\nDa Vinci Colloquium, Dallas, TX\ncolloquium\n\n\n2000-11-01\nVisual Methods for Classification Problem\nSouthern Methodist University, Dallas, TX\ninvited talk\n\n\n2000-07-01\nIssues and Approaches for Visualization of Large Multi-Dimensional Data\nNRCSE/GSP Workshop, Boulder, CO\ninvited talk\n\n\n\n\n\n\n\nAnd another 30 or so delivered before 2000."
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "Graduated and current PhD students",
    "section": "",
    "text": "nameyearfromtopicco-advisorsKrisanat Anukarnsakulchularp2028ThailandVisual methods for exploring multivariate spatio-temporal networks with application to health transportMichael LydeamoreVis Leong2027MalaysiaHeterogeneity effects of mechanical ventilation on respiratory infection caused by multidrug-resistant Acinetobacter Baumannii in the intensive care unitEwilly Liew, Michael Lydeamore, Manjeevan Singh Seera, Poh Yeh HanXuan Nhat Minh Nguyen2027VietnamTBDTim DwyerTina Rashid Jafari2027Tabriz, IranDeveloping High-Dimensional Visualization Methods to Compare Shape Differences Between Two Sets of Financial Seriesfor Statistical TestingFarshid Vahid-AraghiJanith Wanniarachchi2026Colombo, Sri LankaVisualisation for XAIKate Saunders,  Patricia Menendez,  Thiyanga TalagalaHarriet Mason2025Brisbane, AustraliaUncertainty visualisationSarah Goodwin,  Susan VanderplasJayani Lakshika Piyadi Gamage2025Weligama, Sri LankaLooking at Non-Linear Dimension Reductions as Models in the Data SpacePaul Harrison,  Michael Lydeamore,  Thiyanga TalagalaWeihao (Patrick) Li2024Guangzhou, ChinaAdvances in Artificial Intelligence for Data Visualization: Developing Computer Vision Models to Automate Reading of Data PlotsKlaus Ackermann,  Emi Tanaka,  Susan VanderplasH. Sherry Zhang2023Shanghai, ChinaNew tools for visualising and explaining multivariate spatio-temporal dataUrsula Laa,  Nicholas Langrene,  Patricia MenendezNicholas Spyrison2022Dubuque,  IowaInteractive and dynamic visualization of high-dimensional dataKim MarriotSayani Gupta2022Delhi, IndiaVisualization and analysis of probability distributions of large temporal dataRob Hyndman,  Peter ToscasStuart Lee2021Adelaide,  AustraliaExploratory data analysis for high-dimensional biological datasetsPaul Harrison, Matt RitchieEaro Wang2020Chengdu, ChinaTidy tools for supporting fluent workflow in temporal data analysisRob HyndmanNatalia Da Silva2017Montevideo, UruguayBagged projection methods for supervised classification in big dataHeike HofmannLindsay Rutter2016USAVisualization Methods for Genealogical and RNA-sequencing Studies: Pertinence, software, and applicationsAmy TothXiaoyue Cheng2015Qingdao, ChinaInteractive visualization for missing values,  temperoal and areal dataHeike HofmannNiladri Roy-Chowdhury2014Mumbai, IndiaExplorations of the lineup protocol for visual inference: Application to high dimension low sample size problems and metrics to assess the qualityHeike HofmannYihui Xie2013Yichang, ChinaDynamic graphics and reporting for statisticsHeike HofmannMahbubul Majumder2013Dhaka, BangladeshInvestigations into Visual Statistical InferenceHeike HofmannTengfei Yin2013Wuhan, ChinaVisualization of biological data: Infrastructure, design and applicationEve WurteleXiaoyong Sun2010Shandong, ChinaDiagnostics for nonlinear models with application to population pharmacokinetic modelingBasil NikolaiMichael Lawrence2008St Louis, USAInteractive graphics, graphical user interfaces and software interfaces for the analysis of biological experimental data and networksEve WurteleHadley Wickham2008Hamilton,  New ZealandPractical tools for exploring data and modelsHeike HofmannEun-Kyung Lee2003Seoul,  South KoreaProjection pursuit methods for exploratory supervised classificationOzlem Ilk2004Ankara,  TurkeyExploratory multivariate longitudinal data analysis and models for multivariate longitudinal binary dataMarcia Macedo2000Sao Paulo, BrazilModern applied statistics and animal populations in geographically complex landscapes: Data visualization and modelingBrent DanielsonSunhee Kwon1999Seoul, South KoreaClustering in multivariate data: Visualization,  case and variable reduction"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "year\ndegree\ninstitution\n\n\n\n\n1982\nB.Sc./Dip. Ed. (Math/Stat/Biochem)\nUniversity of New England, Armidale, Australia\n\n\n1991\nM.Sc (Statistics)\nRutgers University, New Jersey, USA\n\n\n1993\nPh.D (Statistics)\nRutgers University, New Jersey, USA"
  },
  {
    "objectID": "education.html#degrees",
    "href": "education.html#degrees",
    "title": "Education",
    "section": "",
    "text": "year\ndegree\ninstitution\n\n\n\n\n1982\nB.Sc./Dip. Ed. (Math/Stat/Biochem)\nUniversity of New England, Armidale, Australia\n\n\n1991\nM.Sc (Statistics)\nRutgers University, New Jersey, USA\n\n\n1993\nPh.D (Statistics)\nRutgers University, New Jersey, USA"
  },
  {
    "objectID": "education.html#positions",
    "href": "education.html#positions",
    "title": "Education",
    "section": "Positions",
    "text": "Positions\n\n\n\n\n\n\n\n\nyear\nposition\ninstitution\n\n\n\n\nJul 2021\nDistinguished Professor\nEconometrics and Business Statistics, Monash University\n\n\nJul 2015\nProfessor\nEconometrics and Business Statistics, Monash University\n\n\nMay 2005\nProfessor\nStatistics, Iowa State University\n\n\nMay 1998\nAssociate Professor\nStatistics, Iowa State University\n\n\nAug 1993\nAssistant Professor\nStatistics, Iowa State University"
  },
  {
    "objectID": "education.html#awards",
    "href": "education.html#awards",
    "title": "Education",
    "section": "Awards",
    "text": "Awards\n\nKeynote speaker, IASC-ARS 2023, Sydney, Dec 6-8, 2023\nKeynote speaker, IFCS 2022, Porto, Portugal, Jul 19-23, 2022\nGosset lecture, Irish Statistical Association, Feb 3, 2021\nKeynote speaker, IBC 2020, Seoul, South Korea, Jul 5-10, 2020\nKeynote speaker, 7th International Statistical Ecology Conference, Sydney Jun 22-26, 2020\nElected member, International Statistical Institute 2019\nKeynote speaker, DSSV 2019, Kyoto, Japan, Aug 13-15 2019\nACEMS and Monash Master class, Melbourne, Feb 13, 2019\n2018 ACEMS Impact and Engagement Award, as part of the Non-uniform Monash Business Analytics Team\nBelz lecture, University of Melbourne, Oct 16, 2018\nKeynote speaker, RStudio Conference, San Diego, CA Feb 2-3, 2018\nKeynote speaker, Biometrics by the Border, IBS-AR 2017 Nov 26-30, Kingscliffe, Australia\nKeynote speaker, Young Statisticians Conference, Sep 26-27 2017, Gold Coast, Australia\nOrdinary Member (elected), The R Foundation, 2016\nDean’s Research Award, 2016\nKeynote Speaker, New Zealand Statistical Association Conference, Nov 27-29 2016, Auckland, NZ\nKeynote Speaker, International Statistics Forum, May 27 2016, Renmin University, China\nStatistical Applications and Data Mining, Best paper award 2015\nKeynote Speaker, Bayes on the Beach, Dec 8 2015, Surfers Paradise, Australia\nPlenary Speaker, RSFAS Dec 2 2015 Summer Camp, Murramurang, Australia\nKeynote Speaker, BDVA ’15, September 2015, Hobart.\nAustralia Keynote Speaker, useR! Jul 2015, Aalborg, Denmark\nKeynote Speaker, useR! Jun 2012, Nashville, TN\nLAS Career Research Award, 2012\nInfoVis 2010 Best paper award\nFellow, American Statistical Association, 2005\nRecognized by two outstanding undergraduates, one from Statistics 101 and the other from Statistics 407 as a teacher who was important to them, 2009\nCenter for Excellence in Learning and Teaching Memorable Teacher 2007\nDelta Delta Delta Favorite Teacher Recognition 2005\nUniversity Foundation Award for Early Excellence in Research, 1999"
  },
  {
    "objectID": "education.html#visits",
    "href": "education.html#visits",
    "title": "Education",
    "section": "Visits",
    "text": "Visits\n\n\n\n\n\n\n\ndates\ninstitution\n\n\n\n\nJun 17-21, 2024\nTuring Institute, London, UK\n\n\nMar-Jul, 2023\nEWHA Womens University, Seoul, South Korea\n\n\nJan-May 2008\nIsaac Newton Institute for Mathematical Sciences, Cambridge University, UK\n\n\nFeb-May 2004\nStatistics, Macquarie University, Australia\n\n\nJul 2002\nUniversity of Augsburg, Germany\n\n\nDec 2001\nAT&T Labs, Florham Park, NJ\n\n\nOct 2000\n3D Visual Data Mining Group, University of Aalborg, Denmark\n\n\nApr-Jun 2000\nLucent Technologies, Indian Hill, IL\n\n\nJan-Apr 2000\nBiometricsSA, Adelaide, Australia\n\n\nOct 1999\nPfizer Inc, Groton, CT\n\n\nJun-Jul 1999, Oct- Nov 1998\nNational Research Center for Statistics and the Environment, University of Washington, Seattle, WA\n\n\nJun-Aug 1997, Jul- Sep 1999\nStanford University, CA\n\n\nMar 1996, Mar 1995, Nov 1994\nAT&T Bell Labs, Murray Hill, NJ\n\n\nAug 1994\nHumboldt University, Berlin, Germany\n\n\nJul 1994, Jul 1993, Jul 1991\nCentre for Mathematics and its Applications, Australian National University"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Publications",
    "section": "",
    "text": "packagelast_updatereleasedversiontitledownloadshextsibble30/01/202509/01/20181.1.6Tidy Temporal Data Frames and Tools1,593,225rjtools23/01/202514/11/20221.0.18Preparing, Checking, and Submitting Articles to the 'R Journal'8,040spinifex08/01/202509/04/20190.3.8Manual Tours, Manual Control of Dynamic Projections of NumericMultivariate Data28,597learningtower21/12/202406/09/20211.1.0OECD PISA Datasets from 2000-2022 in an Easy-to-Use Format13,065texor13/12/202408/06/20231.5.3Converting 'LaTeX' 'R Journal' Articles into 'RJ-web-articles'11,668spotoroo19/11/202422/03/20210.1.5Spatiotemporal Clustering of Satellite Hot Spot Data13,523sugarglider24/10/202424/10/20241.0.3Create Glyph-Maps of Spatiotemporal Data1,585rebib15/10/202426/05/20230.5.0Convert and Aggregate Bibliographies5,361woylier01/10/202418/05/20230.0.9Alternative Tour Frame Interpolation Method1,981feasts25/09/202428/08/20190.4.1Feature Extraction and Statistics for Time Series856,894fabletools17/09/202408/08/20190.5.0Core Tools for Packages in the 'fable' Framework1,061,086cassowaryr13/09/202409/08/20222.0.2Compute Scagnostics on Pairs of Numeric Variables in a Data Set7,546cubble27/08/202422/04/20221.0.0A Vector Spatio-Temporal Data Structure for Data Analysis31,667nullabor21/07/202405/10/20120.3.12Tools for Graphical Inference107,410ferrn23/06/202417/03/20210.1.0Facilitate Exploration of touRR optimisatioN11,809brolgar10/05/202416/12/20201.0.1Browse Over Longitudinal Data Graphically and Analytically in R21,213tourr20/04/202406/10/20121.2.0Tour Methods for Multivariate Data Visualisation143,436cardinalR16/04/202416/04/20240.1.1Collection of Data Structures1,860sugrrants12/03/202428/07/20170.2.9Supporting Graphs for Analysing Time Series67,315naniar05/03/202409/08/20171.1.0Data Structures, Summaries, and Visualisations for Missing Data1,548,025quollr05/03/202405/03/20240.1.1Visualising How Nonlinear Dimension Reduction Warps Your Data1,728ggenealogy21/02/202403/03/20151.0.3Visualization Tools for Genealogical Data41,185GGally13/02/202405/10/20122.2.1Extension to 'ggplot2'6,540,837animbook05/12/202305/12/20231.0.0Visualizing Changes in Performance Measures and DemographicAffiliations using Animation3,390tidyindex16/11/202316/11/20230.1.0A Tidy Data Pipeline to Construct, Compare, and Analyse Indexes2,323mulgar25/08/202322/06/20231.0.2Functions for Pre-Processing Data for Multivariate DataVisualisation using Tours4,424DescribeDisplay25/08/202317/10/20120.2.11An Interface to the 'DescribeDisplay' 'GGobi' Plugin54,022sugarbag09/11/202214/06/20190.1.6Create Tessellated Hexagon Maps20,887PPforest09/09/202215/12/20170.1.3Projection Pursuit Classification Forest28,168yowie06/09/202106/09/20210.1.0Longitudinal Wages Data from the National Longitudinal Survey ofYouth 19795,009ozmaps03/08/202106/11/20190.4.5Australia Maps67,112eechidna25/02/202123/05/20161.4.1Exploring Election and Census Highly Informative Data Nationallyfor Australia33,518ggmosaic23/02/202130/12/20160.3.3Mosaic Plots in the 'ggplot2' Framework338,384tsibbletalk02/10/202002/10/20200.1.0Interactive Graphics for Tsibble Objects15,596gravitas25/06/202005/11/20190.1.3Explore Probability Distributions for Bivariate TemporalGranularities23,708quokar10/11/201710/11/20170.1.0Quantile Regression Outlier Diagnostics with K Left Out Analysis19,671geozoo07/05/201606/10/20120.5.1Zoo of Geometric Objects119,709\n\n\n\nThis page is automatically populated using pkgsearch with R with all time downloads."
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "Publications",
    "section": "",
    "text": "packagelast_updatereleasedversiontitledownloadshextsibble30/01/202509/01/20181.1.6Tidy Temporal Data Frames and Tools1,593,225rjtools23/01/202514/11/20221.0.18Preparing, Checking, and Submitting Articles to the 'R Journal'8,040spinifex08/01/202509/04/20190.3.8Manual Tours, Manual Control of Dynamic Projections of NumericMultivariate Data28,597learningtower21/12/202406/09/20211.1.0OECD PISA Datasets from 2000-2022 in an Easy-to-Use Format13,065texor13/12/202408/06/20231.5.3Converting 'LaTeX' 'R Journal' Articles into 'RJ-web-articles'11,668spotoroo19/11/202422/03/20210.1.5Spatiotemporal Clustering of Satellite Hot Spot Data13,523sugarglider24/10/202424/10/20241.0.3Create Glyph-Maps of Spatiotemporal Data1,585rebib15/10/202426/05/20230.5.0Convert and Aggregate Bibliographies5,361woylier01/10/202418/05/20230.0.9Alternative Tour Frame Interpolation Method1,981feasts25/09/202428/08/20190.4.1Feature Extraction and Statistics for Time Series856,894fabletools17/09/202408/08/20190.5.0Core Tools for Packages in the 'fable' Framework1,061,086cassowaryr13/09/202409/08/20222.0.2Compute Scagnostics on Pairs of Numeric Variables in a Data Set7,546cubble27/08/202422/04/20221.0.0A Vector Spatio-Temporal Data Structure for Data Analysis31,667nullabor21/07/202405/10/20120.3.12Tools for Graphical Inference107,410ferrn23/06/202417/03/20210.1.0Facilitate Exploration of touRR optimisatioN11,809brolgar10/05/202416/12/20201.0.1Browse Over Longitudinal Data Graphically and Analytically in R21,213tourr20/04/202406/10/20121.2.0Tour Methods for Multivariate Data Visualisation143,436cardinalR16/04/202416/04/20240.1.1Collection of Data Structures1,860sugrrants12/03/202428/07/20170.2.9Supporting Graphs for Analysing Time Series67,315naniar05/03/202409/08/20171.1.0Data Structures, Summaries, and Visualisations for Missing Data1,548,025quollr05/03/202405/03/20240.1.1Visualising How Nonlinear Dimension Reduction Warps Your Data1,728ggenealogy21/02/202403/03/20151.0.3Visualization Tools for Genealogical Data41,185GGally13/02/202405/10/20122.2.1Extension to 'ggplot2'6,540,837animbook05/12/202305/12/20231.0.0Visualizing Changes in Performance Measures and DemographicAffiliations using Animation3,390tidyindex16/11/202316/11/20230.1.0A Tidy Data Pipeline to Construct, Compare, and Analyse Indexes2,323mulgar25/08/202322/06/20231.0.2Functions for Pre-Processing Data for Multivariate DataVisualisation using Tours4,424DescribeDisplay25/08/202317/10/20120.2.11An Interface to the 'DescribeDisplay' 'GGobi' Plugin54,022sugarbag09/11/202214/06/20190.1.6Create Tessellated Hexagon Maps20,887PPforest09/09/202215/12/20170.1.3Projection Pursuit Classification Forest28,168yowie06/09/202106/09/20210.1.0Longitudinal Wages Data from the National Longitudinal Survey ofYouth 19795,009ozmaps03/08/202106/11/20190.4.5Australia Maps67,112eechidna25/02/202123/05/20161.4.1Exploring Election and Census Highly Informative Data Nationallyfor Australia33,518ggmosaic23/02/202130/12/20160.3.3Mosaic Plots in the 'ggplot2' Framework338,384tsibbletalk02/10/202002/10/20200.1.0Interactive Graphics for Tsibble Objects15,596gravitas25/06/202005/11/20190.1.3Explore Probability Distributions for Bivariate TemporalGranularities23,708quokar10/11/201710/11/20170.1.0Quantile Regression Outlier Diagnostics with K Left Out Analysis19,671geozoo07/05/201606/10/20120.5.1Zoo of Geometric Objects119,709\n\n\n\nThis page is automatically populated using pkgsearch with R with all time downloads."
  },
  {
    "objectID": "software.html#other-software",
    "href": "software.html#other-software",
    "title": "Publications",
    "section": "Other software",
    "text": "Other software\n\nSwayne, D., Temple Lang, D., Cook, D., and Buja, A. (2001-), with Wickham, H., Lawrence, M. and Hofmann, H. (2005-), GGobi: software for exploratory graphical analysis of high-dimensional data. There is an associated R package rggobi which provides a tight coupling of R and ggobi, with command line control of ggobi.\nCook, D., Sutherland, P., Honavar, V., Miller, L., Suarez, M. and Jing Zhang (1999- ). Limn: Visualizing extremely large data sets.\nSutherland, P., Rossini, A., Lumley, T., Dickerson, J., Cox, Z., and Cook, D. (1998-). ORCA: Multivariate Data Viewers using JAVA.\nSwayne, D., Cook, D., and Buja, A. (1990-). XGobi: software for exploratory graphical analysis of high-dimensional data using scatterplot manipulation..\nMajure, J. J., Symanzik, J., and Cook, D. (1996). ArcView 2.1 - XGobi link: software connecting a GIS with dynamic statistical graphics program for multivariate data."
  },
  {
    "objectID": "files/MalaysiaR/components/titleslide.html",
    "href": "files/MalaysiaR/components/titleslide.html",
    "title": ".monash-blue.outline-text[]",
    "section": "",
    "text": "background-color: #006DAE class: middle center hide-slide-number\n\n These slides are viewed best by Chrome and occasionally need to be refreshed if elements did not load properly.\n\n\n.white[Press the right arrow to progress to the next slide!]\n\nbackground-image: url() background-size: cover class: hide-slide-number split-70 title-slide count: false\n.column.shade_black[.content[\n\n\n.monash-blue.outline-text[]\n\n\n\n\n\n.bottom_abs.width100[\n**\n&lt;i class=\"fas  fa-link faa-float animated \"&gt;&lt;/i&gt; https://bit.ly/Cook-MalaysiaR\n ]\n]]"
  },
  {
    "objectID": "files/ACEMS_retreat_2020/components/titleslide.html",
    "href": "files/ACEMS_retreat_2020/components/titleslide.html",
    "title": ".monash-blue.outline-text[]",
    "section": "",
    "text": "background-color: #006DAE class: middle center hide-slide-number\n\n These slides are viewed best by Chrome and occasionally need to be refreshed if elements did not load properly.\n\n\n.white[Press the right arrow to progress to the next slide!]\n\nbackground-image: url() background-size: cover class: hide-slide-number split-70 title-slide count: false\n.column.shade_black[.content[\n\n\n.monash-blue.outline-text[]\n\n\n\n\n\n.bottom_abs.width100[\n**\nDepartment of Econometrics and Business Statistics\n&lt;i class=\"fas  fa-link faa-float animated \"&gt;&lt;/i&gt; http://bit.ly/VicBushFireIgnition\n ]\n]]"
  }
]